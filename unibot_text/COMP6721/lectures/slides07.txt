
layer 1
layer
COMP 6721: AI
Slide 2
Learn Several Filters
sparse (i.e. many
algorithm
57
 Both are 2 layer neural networks that learn to model
68 85 180 214 245 0
Slide 40
 pixel -> edge -> texton -> motif -> part -> object
5. Conclusion
163 158 204 253
x6
2. Autoencoders
61
Train parameters (weights)
 so weight updates get smaller and smaller
gradients)
ur
35
Value = 0-> white …. 255->black
 i.e. Automatic feature learning… (see next few slides)
Fe
Representation
 modify the error function that we minimize to penalize large weights.
 Speech Recognition & Machine Translation (2010+)
representations
Slide 55
A Deep Neural Net
 has many layers of neurons which act as feature detectors
encoders instead of RBM’s
layer first
When we multiply the gradients many times (for
http://cs231n.github.io/convolutional-networks/
2. Time-consuming
18
Many Types of Deep Networks (con’t)
15
3. Multilayered ANNs need lots of labeled data
top of one another – deep belief network
9
 Constrain layer 2 to be
http://yann.lecun.com/exdb/lenet/
number of layers
But keep bi’s sparse (ie. many
A
Feature Learning
learned from the data
60
automatically using unsupervised data
→ Worksheet #6 (“Pooling Layer”)
 Dropout:
 Nb of units in layer 2 < nb
Slide 43
Slide 17
0 0 0
Advantages of Unsupervised Feature
25
Learning a Hierarchy of Features
41
multiplication
Slide 48
zero otherwise.
Slide 63
1. Unsupervised pre-training of neural network using
autoencoder) aka “pretraining the network
b3
Today
1. Standard backpropagation with sigmoid activation function
classification)
3 154 104 235 25 130
7
input to the next layer
496.8 648.5
set of labelled data.
Slide 12
2. Toronto: (Hinton et al.)  Vector Institute
 To learn a representation of the data, we can use:
Examples of learned objects parts from object categories
https://www.amii.ca/
Slide 10
Slide 64
generalize to
Slide 20
data and hand-
1 0.3
 For image recognition
913 851
Slide 13
(eg. linguistics, medical doctors,…)
 speech recognition
 prevents the network from becoming too dependent on any
54
x5
 AlexNet
overfit
37
 As the features are learned in an unsupervised way from a
 Used to:
1. We feed the
layer
To help, we can :
does not scale well with multiple layers
Slide 34
Slide 50
Slide 42
Slide 15
1
Training a Deep NN
Sutskever and Geoff Hinton (U. of Toronto)
 and the weights can become so large as to
0 1 0 1 0 0
Slide 31
a2
Use of unlabelled data to “pretrain” the network
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b46342
 …
 Several strategies:
(27 layers)
Slide 1
computer vision
https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
Slide 23
Pooling Layer
 Image Recognition & Computer Vision (2012+)
Motorbikes
 Training layer by layer on un-labeled data
 Most data is not labeled :(
Architecture of a CNN
Slide 16
“disabled”)
(the last, supervised, layer).
 The weight matrix (filter/kernel) behaves like a filter
Slide 6
 now : Access to DL methods, code and frameworks
 Stack:
and expensive
Slide 29
 Need for lots training data…
layer 2
Learned
486.1 731.2 736
3.567%
x2
learn a representation of the data (eg.
Slide 45
Feature Learning:
31 → Worksheet #6 (“Autoencoder”)
(7×7)          W (3×3) with stride =2          C  (3×3)
26
5. Deep Learning for NLP
 Trivial… unless, we
56 3 8 175
 where f(w) grows larger as the weights grow larger and λ is the
Convolution Hyper-parameters
 Finish off with:
Backprop error
x1
 Top Layer: Cat or Dog
input layer has 200*200*3 = 120,000 weights
3. Open Access to resources
 First work that popularized CNNs for
 now: developed method for training, better activation functions,
Slide 57
Slide 30
tasks…  deep learning→
Image of a 4 in grey scale
 now: use of GPU’s which are optimized for very fast matrix
 Average pooling
121.9 9.9 417.5
other domains
Padding
Image Captioning: Better than humans?
Feature
35 24 204 113 109 221
 For NLP
Slide 62
 used to read zip codes, digits, etc.
(eg. edge detection, colors,
What Types of Features?
Input
46
https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
Slide 25
Google Translate
1. Needs expert
Faces Cars Elephants Chairs
 Mid 2000’s: Geoffrey Hinton trains a deep network by:
 Then, using back propagation to fine tune weights on
→ Worksheet #6
Slide 36
3. Features learned
0 2 1 0 1 0
Use pretrained
variety of OTHER classification
16
(7×7)          W (3×3) with stride =1          C  (5×5)
Slide 51
15 253 225 159 78 233
 Speech Recognition & Machine Translation (2010+)
 gradients shrink exponentially with the
8
one neuron.
28
i.e. the encoded x
0 0 1 1 2 0
OR
1. Deep Belief Networks
9 picked up ;-)
 character -> word ->  constituents -> clause -> sentence -> discourse
“see” some visual feature that is useful to identify the object (the final
“learn by heart”
2. Supervised training with labeled data using features
 Use a filter (aka kernel) that “convolves” on the image
http://vectorinstitute.ai/
parts
 Much more unlabeled data available than labeled data:
 Ex. a horizontal line, a blotch of some color, a circle…
– Y. Bengio
(pixels)
this layer
Automatic Feature Learning
input
objects
The Google “Inception” deep neural network architecture for image recognition
edges
i.e. the encoded a
https://mila.quebec/en/
Slide from Yoshua Bengio, 2015 6
 i.e. learn the identity
2. Eg. color image of 200x200x 3channels (RGB)
 A fully connected layer at the end for the final classification
Autoencoders
new output
labeled data
Skype Translator
 aka. unsupervised learning of features
2. We linearize the image ==> We lose spatial
 In 2012 significantly outperformed all teams
Question Answering
regularization strength
better architectures….
Google now
701.8 743
Slide 22
17
Learn the Filters
Initial Drawbacks
105 2 3 69
→ Worksheet #6 (“Activation Map”)
Initial Drawbacks (2)
https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/
34
Train parameters
 and weights of early layers change very
wheel
with unsupervised data”
Slide 44
2
data.
 Pooling Layers
620.4 268.2 443.6
0 9 0 0 1 0
Slide 7
subject to ci’s being sparse.
object
representations can be used in
function.
 The network is trained
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6
crafted features
network the raw data
1. Standard gradient-based backpropagation does not scale well with
Slide 49
12
similar tasks.
 sample  spectral band -> sound -> … phone -> phoneme -> word→
Slide 53
48
18 54 51 239 244 188
19
2. Padding
1 0 1
Deep Learning in the Academic Press (2012-2015)
Slide 28
New (compressed) representation of
608 913 713 657
 developed by Alex Krizhevsky, Ilya
impose constraints:
https://
General Architecture of a Deep Network
 So that we reduce the number of parameters of the
2. GPU computing
Figure from Yoshua Bengio, 2015
features)
can be re-used in
learned from above with a standard classifier
27
a1
Deep Learning = Machine learning algorithms based on
 Humans learn initially from unlabeled examples
Deep Learning
“Non”-Motorbikes
49
 Filter = small weight matrix to learn
Slide 4
network
http://www.cormix.info/images/RuleTreeExample.jpg
Autoencoder
https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
29
1. Motivation
Slide 59
163 8 4 142
a) Use other activation
(output)
Slide 24
9 not picked up ;-(
Actual images
(input)
1. Montreal: (Bengio et al.)   MILA
Use [c1, c3, c3] as representation to
Initial Drawbacks (1)
 Like the human brain …
then this
feed to supervised learning algorithm
an
 Large network -> lots of parameters -> increased capacity to
EACH of the (non-output) layers is trained to
1 1
connections are
 …
requires labeled
https://www.strong.io/blog-images/movie-posters/Slide6.png 20
24
Slide 39
a3
Slide 19
Why now?
 Natural Language Processing (2014+)
47
0 0 0 0 0 0
regular ANN with a smaller
0 1 1 2
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43
 Convolutional Layers
 Each layer learns more abstract features that are then combined /
 Eg. Websites, Books, Videos, Pictures
each layer),  it can lead to …
 Each level creates new features from combinations of features from
Object recognition Self driving cars
multiple layers…
Figure from Y LeCun 23
33
139.6 20.4 377.7
Slide 37
(i.e. set bounds on the
429 505 686 856
 Developed by Yann LeCun in the 1990’s
https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html
0 1 0
 Weight of early layers change too slowly (no learning)
3. Training a Deep Neural Network
92.6 16.1 195.4
https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/
finally
x3
0 1
features
New representation for the
History of AI
36
Rules written by experts
c1
63
to output the input
 Backpropagation did not work / overfitting…
38
 multiplying gradients could also make them
Slide 8
at the ImageNet ILSVRC challenge
283.9 22.9 349.5
2. The features are
c3
smaller set of labelled
zeros).
information
Slide 58
Automatic
32
layer 3
Successful CNN Networks
14
Learning
Slide 65
for input.
44
2. Feature Learning
learning multiple levels of representation / abstraction.
 Neural networks take very very long to train… (days, weeks)
Slide 27
 Stacking  Restricted Boltzmann Machines (RBM’s) on
network and hence avoid overfitting.
Slide from Y. LeCun
Artificial Intelligence:
51
b) Exploding gradient problem:
 For speech:
Slide 18
e
 To reduce the size of the activation maps
 Natural Language Processing (2014+)
 E.g. to classify an image of a cat:
6. Conclusion
Manual Extraction of Features
Slide 38
B
x4
Eg: Learning Image Features
the input to be fed to the next layer
Classic ML
“wheel”
 Eg. an ANN, SVM, …
Slide 14
55 121 75 78 95 88
 Large network -> lots of parameters ->

 Middle Layers: Fur patterns, eyes, ears
representations to feed
45
e”
53
Many Types of Neural Networks
52
History of AI Rules learns via the data ;-)
Convolutional Layer
increased capacity to “learn by heart”
58
 The network learns the values of the filter(s) that activate when they
a regular ANN with a
b2
CNNs for Image Processing
 so weight updates get larger and larger
40
Slide 61
Slide 60
--> huge number of parameters, can easily
 Standard input of the image in the ANN:
dl
at
4. CNNs for Image Processing
5x5
 detecting more and more abstract features as you go up
 Deep Learning is thriving !
knowledge
curated)
Slide 35
b) Do “gradient clipping”
Learned features /
Initial Drawbacks (3)
Slide 3
13
65
Slide 11
 image processing
 Each level is more abstract than the ones below (hierarchy of
(not feature-
261 792 412 640
Image Captioning (deep vision + deep NLP)
223.8 13.6 341.4
Conclusion
texture,…)
 keep a neuron active with some probability p or setting it to
Slide 5
22
+1
1. Stride
 These features are organized into multiple levels
 vision
Slide 46
252 3 8 40
the level below
Learning
Rules AND
3. Edmonton: (Sutton et al.) AMII
 Max pooling
34 7 7 163
representations to feed a
Advantages of Unsupervised
126 1 2 178
Slide 54
55
11
 LeNet
4th year
22 222 74 180
 Skype Translator
Slide 56
grow exponentially.
 Regularization:
output
their inputs

Classic ML,
 No need for manual feature engineering
 Canada is a world leader in Deep Learning
86 4 8 184
c2
--> in a fully connected ANN, a neuron of the
handle
Feedforward input
 Solutions:
56
Slide 32
50
overflow and result in NaN values
www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
b1
0.5 2
2. Overfitting
219.2 23.9 147.2
Slide from Yoshua Bengio, 2015 5
of input units (learn
39
Deep Learning in the News (2012-2014)
representation)
Major Breakthroughs
different and larger dataset, less risk of over-fitting
many slides from:  Y. Bengio, A. Ng and Y. LeCun
2 1 0 1
new input
Stride
30
But: features identified by the experts
Slide 47
Example of a CNN
slowly and network learns very very slowly
…
59
(“Autoencoder Activation”)
i.e. learn more and more abstract feature
 Higher Layers: Body, head, legs
1 0 1 0
3.581%
62
Slide 26
http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
“h
Slide 41
4
 Solution: “pre-train” the network with features found
 now: we have massive amounts + unsupervised pre-training
633 653 851 751
composed into higher-level features automatically
Slide 33
functions…
 now : Fast turnaround from idea to implementation
Slide 21
Slide 9
 2006: Yoshua Bengio et al. does something similar using auto-
learned by the
compressed
 So we create 1 activation map per filter
new output
eg. handle, wheel, … With Automatic
CNNs = Convolutional Neural Networks
10
unlabeled data
1. Basic science
1. 1 pixel = 1 input.
Feature 1
Deep Learning in the News (2013)
 Eg. Babies learn to talk/recognize objects without labeled data
792 856
(eg. linguistics, medical doctors,…)
 Bottom Layers: Edge detectors, curves, corners straight lines
train this
filter should pick up high values surrounded by low values
9 0 0 1
3. Does not
New representation
21
input layer
64
3
 natural language processing
 First successful applications of CNNs
Slide 52
a) Vanishing gradient problem:
