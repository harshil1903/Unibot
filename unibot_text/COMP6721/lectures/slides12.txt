
layer 1
as vectors
COMP 6721: AI
 The cat/kitty/dog hunts for mice.
http://ahogrammer.com/2017/03/22/why-is-word-embeddings-important-for-natural-language-processing/
ili
sparse (i.e. many
source: Luger (2005)
Machine Translation (NMT) )
 Neural network structures
Word2Vec – Weights W’
x6
https://translate.google.ca/
Results
 aka. word representations
ba
η: learning rate
16 17 18 19 20 ...
https://code.google.com/archive/p/word2vec/
ro
ur
35
d
Word2Vec – FeedForward next data
ve
 and only keep W’, these are your word embeddings!
 Speech Recognition & Machine Translation (2010+)
r
3844
e-
18
15
V
9
23
 Constrain layer 2 to be
 W is a VxN matrix…
A
Word2Vec – Weights W
 i.e., word embeddings
2. So we can use the contexts to guess the word (or vice-versa).
Predict
 Vector representation of words
O= output 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 0.2837 0.1883 0.0686 0.0050 …
 Nb of units in layer 2 < nb
 Networks with loops in them, allowing information to persist.
“t
Matrix W’
25
11 12 13 14 15 ...
41
instance 1 Context +1 cat 0 0 0 0 0 0 0 0 1 ...
The brown furry cat chases the mouse
31
Today
7
ab
6 7 8 9 10 ...
 Basic Idea:
1 2 3 4 5 ...
 <after a bit of math>, we get:
 Length = | vocab |
http://www.claudiobellei.com/2018/01/06/backprop-word2vec/ 21
decision/output
~30 million books, 1850-1990, Google Books data
To
Train embeddings on old books to study
2. Then take the average
position n+t
x5
https://vsubhashini.github.io/s2vt.html
Picture from (Antol et al., 2015) 43
37
layer
1
weight only of the context word since its previous weights lead to an error.
a2
 decision/output from the past can influence current
 …
Inst
2 brown furry chases the cat
context (brown furry chases
 Image Recognition & Computer Vision (2012+)
 Image Captioning
 Word embeddings:
“disabled”)
ho
layer 2
f
“b
x2
3. Train an ANN to guess a word given its context (or vice-versa)
or
 Trivial… unless, we
“f
ct
26
“A word is known by the
training set
http://deeplearning.cs.toronto.edu/i2t
Backprop error
x1
Context
with only 3 layers:
1. One input layer
 https://code.google.com/archive/p/word2vec
3. Deep Learning for NLP
n
 As of July 2017, uses an RNN + word embeddings (called Neural
 But we will take the learned weights as the word embeddings
Image Captioning: Better than humans?
1. Introduction
 However, this does not represent word meaning ;-(
Stages of NLU
 [0, 0, 0, 1, 0, 0, 0, . . .]
 To do NLP with neural networks, words need to be represented
...
0.32 0.33 0.34 0.38 0.36 0.37 0.25 0.55 0.56 0.41 …
"apricot"
 A CNN is used to encode the image.
42
 John’s cat/kitty purrs.
 Recursive Neural Networks
0.17 0.40 0.60 0.25 0.26 0.50 0.35 0.37 0.30 0.55 …
the word
Compute Error of output layer
should have similar vector representations
ty
// target = 1 hot
 The classifier will be built!
Word2vec Models
16
Visual Question Answering
2. Skip-gram: given a
y1 y2 y3 y4 y5 y6 y7 y8 y9 y10
2 Context -2 brown 0 0 0 0 0 1 0 0 0 …
(eg, n= 50 to 300)
28
i.e. the encoded x
Backpropagate errors to adjust W and W’
Output: natural language answer
OR
of the target
Context +2 the 0 1 0 0 0 0 0 0 0 ……
-
context words, guess
fo
 Like a point in n-dimensional space
word +2
2 Context +1 chases 0 0 0 0 0 0 0 0 1 …
1 the brown cat chases furry
Machine Translation
”
1. Calculate the output of each of the N hidden nodes
 We would like:
 use unsupervised texts from the Web
its surrounding word
3. One output layer.
input
goal: predict the probability of
Word2Vec – Creating the Data Set
Conversational Agents
 i.e. learn the identity
representation
output layer 43.7550 46.0750 43.1300 46.5950 44.8300 46.8250 47.2600 46.8500 45.8400 43.2200 …
2. Word Embeddings
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
Input: an image + a natural language question
2. One hidden layer and
Deep Learning for NLP
... ... ... ... ... ...
 In the end, we do not actually care about this task
 CNNs + RNNs + word embeddings
T= target 0 0 0 0 0 0 1 0 0 0 …
Embedding
http://rohanvarma.me/Word2Vec/
Question Answering
20
for airplane
0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.50 0.30 …
“c
41 42 43 44 45 ...
softmax(y_i) 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 0.2837 0.1883 0.0686 0.0050 … 1.0000
Deep Learning for NLP
 Then, we throw it away ! (yes, we do!)
17
banana
instance 1 Context -2 the 0 1 0 0 0 0 0 0 0 ...
 Very fast to train
 Dialogue Systems
 1 in the position of the word id, the rest are 0
… P
 Visual Question Answering
34
t
1 Context -1 brown 0 0 0 0 0 1 0 0 0 …
2
=
function.
 The network is trained
Deep learning models for  NLP use:
26 27 28 29 30 ...
V = size of vocabulary
 predict rather than count
ca
the input word, not for all elements of W. Remember that the input words
similar meanings
12
hand-labeled supervision !
instance 1 Context -1 brown 0 0 0 0 0 1 0 0 0 ...
To Predict
Pr
 A RNN is used to encode the sentence.
19
remember the training set:
I’m not leaving ‘till I get my embeddings!
Used in Machine Translation
s”
(furry) in the
New (compressed) representation of
 cat/kitty/dog ... to have similar representations
impose constraints:
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b4638
Word History through Embeddings
languages but with
Word2Vec- Get the embeddings
27
a1
Word2Vec models
the).
Autoencoder
Word2Vec – Input to the Network
5 chases the inside the mouse
1. Motivation
… … … … … … … … … … …
 The output is conditioned on both image and textual inputs.
2 Context -1 furry 0 0 0 0 1 0 0 0 0 …
(output)
ry
36 37 38 39 40 ...
C
(input)
 Idea:
Recurrent Neural Networks
connections are
 …
“
ha
instance 1 Context +2 chases 0 0 0 0 1 0 0 0 0 ...
24
a3
Demo Website
bi
47
word, guess one of
Note: there is no activation function.
3 furry cat the mouse chases
33
 After many iterations of feedforward, backpropagation on the entire
those in +/- 2 word window
0.70 0.13 0.14 0.60 0.16 0.17 0.60 0.19 0.20 0.21 …
 Convolutional Networks (CNNs)
output layer 43.7550 46.0750 43.1300 46.5950 44.8300 46.8250 47.2600 46.8500 45.8400 43.2200 … sum
(i.e. translations)
31 32 33 34 35 ...
x3
vector[Rome] ~ vector[Madrid] – vector[Spain] + vector[Italy]
Input word
 Adjust W’ and W using backpropagation
 Similar words such as python and ruby
 Represent each word by a vector of fixed dimensions
update only the wij for the inputs where x1 =1
// use softmax function
History of AI
36
 Output sequence: words in the target language
to output the input
lit
http://www.stokastik.in/understanding-word-vectors-and-word2vec/
38
C = size of context (2 words before + 2 after)
Turn dot product into probabilities
Word2vec
1 Context +2 the 0 1 0 0 0 0 0 0 0 …
h1 h2 h3 h4 h5 ...
32
layer 3
 Trained on Google News dataset (about 100 billion words).
 The brown furry cat/kitty/dog  is eating.
14
Word2Vec: CBOW Model
similar vectors
h1 h2 h3 h4 h5
Slide from Yoshua Bengio, 2015 4
Instance Context
Artificial Intelligence:
changes in word meaning
 Input sequence: words in a source sentence
1 Context -2 the 0 1 0 0 0 0 0 0 0 …
 Natural Language Processing (2014+)
o
1. Similar words should have similar contexts (surrounding words)
Assume context words are
B
Output of hi is just the dot product
 Popular embedding method
x4
Cool Applications
the input to be fed to the next layer
remember, we did all this to get embeddings...
company it keeps” – J. R. Firth
.
 e.g., A word at position n can influence a word/decision at
https://visualdialog.org/

45
word -1
O-T 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 -0.7163 0.1883 0.0686 0.0050 …
Many Types of Neural Networks
 W’ is a NxV matrix…
ance
 However, similarity/distance between all
1 Context +2 chases 0 0 0 1 0 0 0 0 0
“one hot vectors” is the same
1. CBOW:  given
40
y
he
e^y_i 1.01E+19 1.02E+20 5.38E+18 1.72E+20 2.95E+19 2.17E+20 3.35E+20 2.22E+20 8.09E+19 5.89E+18 … 1.180E+21
 See: https://code.google.com/archive/p/word2vec/
on
0.12 0.50 0.14 0.15 0.50 0.30 0.18 0.20 0.20 0.21 …
at
http://visualqa.csail.mit.edu/
Visual Question Answering
The brown furry cat chases the mouse inside the house.
 Train a classifier on a binary prediction task:
n”
 Code available on the web
word -2
N (nb of nodes in hidden layer, i.e. size of the embedding)
 Google Translate
13
are represented by
Multilingual Word Embeddings
Embedding for
t1 t2 t3 t4 t5 t6 t7 t8 t9 t10
Image Captioning (deep vision + deep NLP)
training set ...
 Traditional approach: “one hot vector”
Words in different
 Is w likely to show up near “cat"?
vector[swimming] ~ vector[swam]  - vector[walked] + vector[walking]
The weight updates are ONLY done on the "rows" of W that correspond to
46 47 48 49 50 ...
22
+1
for zoo
 Use as training set readily available texts, so no need for
http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ 46
word +1
of
// to turn the vector above into
11
are represented as 1-hot vectors, so only the weights of the word that has
 Instead of counting how often each word w occurs near
21 22 23 24 25 ...
 Weight Matrix W’ between hidden & output layer
output

se
// (that nicely add up to 1)
translation, language modelling, ….
29http://www.stokastik.in/understanding-word-vectors-and-word2vec/
Feedforward input
Project 300 dimensions down into 2
for each context word
Slide from Yoshua Bengio, 2015 5
of input units (learn
Uses a shallow neural network
39
Deep Learning in the News (2012-2014)
representation)
// a vector of probabilities
=.
Major Breakthroughs
many slides from:  Y. Bengio, A. Ng and Y. LeCun
dot product with matrix W’
30
 cat/orange/train/python… to have dissimilar representations
Word2Vec – Feedforward
…
ob
 Recurrent Neural Networks (RNNs)
w
almost...
average of dot products
N (size of embeddings)
t”
N = size of the embedding that we want
 Binary vector
 Weight Matrix W between input & hidden
1 Context +1 cat 1 0 0 0 0 0 0 0 0 …
Word Embeddings
compressed
4 cat chases mouse inside the
10
 RNNs + word embeddings
a target word (cat) given a
Deep Learning in the News (2013)
N
(i.e. number of neurons in the hidden layer)
C: size of context (eg 4)
 Feed forward average of hidden neurons and do
 Initially random but modified via backprop
Word2Vec has 2 models:
6 the mouse the house inside
 Iterate feedforward/ backprop until error is minimized
input layer
a "1" are updated.  This makes intuitive sense, as we want to update the
vector[queen] ~ vector[king]  - vector[man] + vector[woman]
3
6
24 25 26 27 28 …
 Video to Text Descriptions
 To model sequences of decisions, such as machine
