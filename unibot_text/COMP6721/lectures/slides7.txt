
2. GPU computing
Deep Learning in the News (2013)
 Deep Learning is thriving !
15
Slide 35
e”
different and larger dataset, less risk of over-fitting
“disabled”)
a3
connections are
input
Faces Cars Elephants Chairs
features
Slide 29
dl
Image of a 4 in grey scale
17
COMP 6721: AI
x6
0 1
Slide 30
Feature
51
network
 Training layer by layer on un-labeled data
64
https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html
3.581%
texture,…)
1. 1 pixel = 1 input.
automatically using unsupervised data
1 1
then this
Slide 53
Slide 4
86 4 8 184
Question Answering
train this
1 0.3
Architecture of a CNN
 Dropout:
layer 3
1. Basic science
Slide 62
i.e. the encoded a
OR
18 54 51 239 244 188
Slide 31
13
A Deep Neural Net
http://cs231n.github.io/convolutional-networks/
https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
Slide 57
--> in a fully connected ANN, a neuron of the
to output the input
www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
 keep a neuron active with some probability p or setting it to
 Solution: “pre-train” the network with features found
Examples of learned objects parts from object categories
layer
http://yann.lecun.com/exdb/lenet/
feed to supervised learning algorithm
Why now?
48
 Solutions:
multiplication
4. CNNs for Image Processing
https://
 The network is trained
increased capacity to “learn by heart”
“wheel”
 Natural Language Processing (2014+)
set of labelled data.
2. Supervised training with labeled data using features
9 picked up ;-)
0 0 0
But keep bi’s sparse (ie. many
0 1 0 1 0 0
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b46342
 Speech Recognition & Machine Translation (2010+)
(i.e. set bounds on the
35 24 204 113 109 221
Slide 46
39
e
 detecting more and more abstract features as you go up
 Image Recognition & Computer Vision (2012+)
Slide 12
56
other domains
1 0 1
37
Autoencoders
Use pretrained
 Humans learn initially from unlabeled examples
633 653 851 751
Rules AND
(input)
92.6 16.1 195.4
EACH of the (non-output) layers is trained to
Slide 51
3 154 104 235 25 130
 aka. unsupervised learning of features
105 2 3 69
2. Toronto: (Hinton et al.)  Vector Institute
19
Learning
4
Slide 37
b3
http://www.cormix.info/images/RuleTreeExample.jpg
of input units (learn
40
Slide 16
22
283.9 22.9 349.5
Fe
Slide 25
compressed
1 0 1 0
similar tasks.
can be re-used in
“Non”-Motorbikes
 so weight updates get smaller and smaller
121.9 9.9 417.5
c3
12
5. Deep Learning for NLP
 i.e. learn the identity
Slide 10
But: features identified by the experts
5. Conclusion
composed into higher-level features automatically
 now : Fast turnaround from idea to implementation
2. Eg. color image of 200x200x 3channels (RGB)
Slide from Yoshua Bengio, 2015 6
https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111
Slide 17
0 9 0 0 1 0
 Ex. a horizontal line, a blotch of some color, a circle…
Slide 56
Advantages of Unsupervised Feature
http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
 modify the error function that we minimize to penalize large weights.
(7×7)          W (3×3) with stride =1          C  (5×5)
Slide 65
2. Time-consuming
Slide 43
Classic ML,
Slide from Y. LeCun
3. Edmonton: (Sutton et al.) AMII
Figure from Yoshua Bengio, 2015
representations to feed
3. Multilayered ANNs need lots of labeled data
learn a representation of the data (eg.
34 7 7 163
layer 1
x1
(eg. linguistics, medical doctors,…)
 For speech:
 Skype Translator
2. Overfitting
network the raw data
Slide 27
Slide 9
 pixel -> edge -> texton -> motif -> part -> object
output
impose constraints:
41
496.8 648.5
“h
 Bottom Layers: Edge detectors, curves, corners straight lines
their inputs
i.e. the encoded x
Feedforward input
 Canada is a world leader in Deep Learning
 so weight updates get larger and larger
Object recognition Self driving cars
2
55
3. Training a Deep Neural Network
30
25
classification)
 Trivial… unless, we
Input
139.6 20.4 377.7
21
b1
autoencoder) aka “pretraining the network
 now: use of GPU’s which are optimized for very fast matrix
 prevents the network from becoming too dependent on any
objects
 Used to:
the input to be fed to the next layer
regularization strength
 character -> word ->  constituents -> clause -> sentence -> discourse
61
26
 Average pooling
913 851
 Each level creates new features from combinations of features from
Slide 24
 developed by Alex Krizhevsky, Ilya
3. Open Access to resources
Slide 7
 Most data is not labeled :(
3. Features learned
does not scale well with multiple layers
 So we create 1 activation map per filter
2. Padding
1
(output)
3. Does not
Example of a CNN
Slide 40
Train parameters (weights)
Manual Extraction of Features
68 85 180 214 245 0
Slide 38
input layer
 …
Slide 21
 Weight of early layers change too slowly (no learning)
Actual images
22 222 74 180
Slide 39
Slide 61
45
18
0 1 0
 Filter = small weight matrix to learn
layer 2
126 1 2 178
Feature 1
(not feature-
36
31 → Worksheet #6 (“Autoencoder”)
one neuron.
https://www.strong.io/blog-images/movie-posters/Slide6.png 20
 Developed by Yann LeCun in the 1990’s
15 253 225 159 78 233
new input
 Much more unlabeled data available than labeled data:
Slide 34
Learned features /
→ Worksheet #6 (“Activation Map”)
Slide 19
x5
1. Stride
better architectures….
10
(eg. linguistics, medical doctors,…)
Slide 15
History of AI Rules learns via the data ;-)
 Need for lots training data…
b) Exploding gradient problem:
information
…
curated)
Eg: Learning Image Features
To help, we can :
 Like the human brain …
 Several strategies:
7
 Each level is more abstract than the ones below (hierarchy of
Conclusion
Skype Translator
Convolution Hyper-parameters
 For image recognition
New representation
ur
Slide 28
learned by the
252 3 8 40
Slide 64
Rules written by experts
Classic ML
429 505 686 856
new output
 These features are organized into multiple levels
→ Worksheet #6 (“Pooling Layer”)
parts
Initial Drawbacks
Learn the Filters
labeled data
many slides from:  Y. Bengio, A. Ng and Y. LeCun
Slide 54
Slide 36
Slide 33
representations to feed a
 Eg. an ANN, SVM, …
Autoencoder
representations can be used in
 Eg. Babies learn to talk/recognize objects without labeled data
52
Slide 11
--> huge number of parameters, can easily
Slide 52
for input.
representation)
 Speech Recognition & Machine Translation (2010+)
Slide 63
x2
Many Types of Deep Networks (con’t)
Automatic Feature Learning
Backprop error
CNNs for Image Processing
9 not picked up ;-(
 and weights of early layers change very
0.5 2
http://vectorinstitute.ai/
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43
29
Train parameters
Slide 18
2. Autoencoders
the level below
2 1 0 1
3.567%
at the ImageNet ILSVRC challenge

Slide 5
 …
6. Conclusion
1. Montreal: (Bengio et al.)   MILA
eg. handle, wheel, … With Automatic
network and hence avoid overfitting.
(“Autoencoder Activation”)
The Google “Inception” deep neural network architecture for image recognition
Advantages of Unsupervised
Use [c1, c3, c3] as representation to
– Y. Bengio
Slide 32
60
features)
4th year
Pooling Layer
 Convolutional Layers
Value = 0-> white …. 255->black
 Large network -> lots of parameters ->
(7×7)          W (3×3) with stride =2          C  (3×3)
 Pooling Layers
 speech recognition
50
at
 As the features are learned in an unsupervised way from a
layer first
9 0 0 1
16
Deep Learning = Machine learning algorithms based on
History of AI
46
requires labeled
Slide 50
65
grow exponentially.
Slide 1
 The network learns the values of the filter(s) that activate when they
 2006: Yoshua Bengio et al. does something similar using auto-
Slide 49
 sample  spectral band -> sound -> … phone -> phoneme -> word→
11
new output
What Types of Features?
this layer
59
b) Do “gradient clipping”
Training a Deep NN
1. Unsupervised pre-training of neural network using
58
 No need for manual feature engineering
number of layers
edges
223.8 13.6 341.4
https://www.amii.ca/
3
 where f(w) grows larger as the weights grow larger and λ is the
 Natural Language Processing (2014+)
function.
 For NLP
Learning
0 0 0 0 0 0
computer vision
data and hand-
zeros).
with unsupervised data”
layer
B
1. Deep Belief Networks
2. We linearize the image ==> We lose spatial
 Use a filter (aka kernel) that “convolves” on the image
multiple layers…
 Constrain layer 2 to be
x4
 Eg. Websites, Books, Videos, Pictures
variety of OTHER classification
sparse (i.e. many
Slide 2
 used to read zip codes, digits, etc.
62
Learned
 now: we have massive amounts + unsupervised pre-training
gradients)
 Large network -> lots of parameters -> increased capacity to
learned from the data
Slide 22
Image Captioning: Better than humans?
 First successful applications of CNNs
 natural language processing
0 2 1 0 1 0
Slide 44
generalize to

14
 Finish off with:
Google Translate
Artificial Intelligence:
 vision
 Nb of units in layer 2 < nb
701.8 743
“learn by heart”
wheel
handle
792 856
Automatic
28
8
data.
General Architecture of a Deep Network
smaller set of labelled
When we multiply the gradients many times (for
slowly and network learns very very slowly
input to the next layer
encoders instead of RBM’s
A
44
 gradients shrink exponentially with the
 Stack:
Slide 47
https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-c
Slide 23
an
and expensive
Slide 59
Slide 58
(pixels)
c1
New (compressed) representation of
 multiplying gradients could also make them
Slide 20
 First work that popularized CNNs for
 has many layers of neurons which act as feature detectors
https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6
learned from above with a standard classifier
 So that we reduce the number of parameters of the
32
Sutskever and Geoff Hinton (U. of Toronto)
Slide 45
 now: developed method for training, better activation functions,
1. Standard gradient-based backpropagation does not scale well with
regular ANN with a smaller
algorithm
overflow and result in NaN values
Figure from Y LeCun 23
subject to ci’s being sparse.
Feature Learning
input layer has 200*200*3 = 120,000 weights
 Then, using back propagation to fine tune weights on
0 1 1 2
Initial Drawbacks (1)
 LeNet
 Regularization:
object
486.1 731.2 736
a) Use other activation
 and the weights can become so large as to
56 3 8 175
b2
Learning a Hierarchy of Features
1. We feed the
Image Captioning (deep vision + deep NLP)
Initial Drawbacks (3)
 Middle Layers: Fur patterns, eyes, ears
Use of unlabelled data to “pretrain” the network
Slide 8
55 121 75 78 95 88
1. Needs expert
x3
Slide 55
38
54
https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/
Deep Learning in the News (2012-2014)
crafted features
Today
Deep Learning in the Academic Press (2012-2015)
New representation for the
+1
261 792 412 640
c2
i.e. learn more and more abstract feature
 Top Layer: Cat or Dog
Learn Several Filters
Slide 48
163 8 4 142
608 913 713 657
CNNs = Convolutional Neural Networks
top of one another – deep belief network
 To reduce the size of the activation maps
27
 Standard input of the image in the ANN:
 image processing
Slide from Yoshua Bengio, 2015 5
53
functions…
 E.g. to classify an image of a cat:
(the last, supervised, layer).
Slide 6
219.2 23.9 147.2
 The weight matrix (filter/kernel) behaves like a filter
filter should pick up high values surrounded by low values
Slide 42
Initial Drawbacks (2)
a1
representations
 Both are 2 layer neural networks that learn to model
learning multiple levels of representation / abstraction.
49
Motorbikes
Padding
 Each layer learns more abstract features that are then combined /
→ Worksheet #6
620.4 268.2 443.6
 Stacking  Restricted Boltzmann Machines (RBM’s) on
0 0 1 1 2 0
https://mila.quebec/en/
zero otherwise.
Successful CNN Networks
overfit
a regular ANN with a
Slide 41
(eg. edge detection, colors,
Deep Learning
tasks…  deep learning→
 Higher Layers: Body, head, legs
57
2. The features are
 now : Access to DL methods, code and frameworks
(27 layers)
 Mid 2000’s: Geoffrey Hinton trains a deep network by:
Slide 3
a) Vanishing gradient problem:
Major Breakthroughs
Stride
Google now
each layer),  it can lead to …
unlabeled data
1. Motivation
63
1. Standard backpropagation with sigmoid activation function
24
34
Feature Learning:
finally
 Neural networks take very very long to train… (days, weeks)
Many Types of Neural Networks
“see” some visual feature that is useful to identify the object (the final
 To learn a representation of the data, we can use:
a2
5x5
 In 2012 significantly outperformed all teams
163 158 204 253
Slide 14
Slide 60
35
 A fully connected layer at the end for the final classification
9
Convolutional Layer
 Backpropagation did not work / overfitting…
 AlexNet
Slide 26
Representation
2. Feature Learning
33
knowledge
47
Slide 13
 Max pooling
 i.e. Automatic feature learning… (see next few slides)
https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/
