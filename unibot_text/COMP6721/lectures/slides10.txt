
 prepositions, determiners, pronouns, conjunctions, …
 top-down (goal-directed): grammar --> words
 “come across some men” --> prob = 0
Slide 75
Det-N-PP Det-N …
9. S
Add-one, more formally
depth first
Example 1:
CONTRAST This is good, but this is better.
Add-delta Smoothing
Slide 72
71
Slide 2
S2: How to wreck a nice beach. ?
 Product of the probabilities of the rules used in subtrees
Example
Where is Citizen Kane playing in
Kingdom
RESULT  Click on the button, the red light will blink.
80

add
“I am going to make a collect …”

57
1j
Slide 40
source: Luger (2005)
Naïve Bayes classifier
 better than add-one, but still…
 John visited Paris.  He bought Mary some expensive
61
Probability of the possible
Where: Gates 159
with a 20 000 word vocabulary:
 Robust (ex. forgot a comma, a word… still OK)
 <s>I eat <s> I sleep <s>
 i.e., Meaning of combination of words
V of surrounding words
environment?
 Feature vector V contains:
81
:note
(circa 2010-today)

3. Smooth your model (see later)
 Bush Wins on Budget, but More Lies Ahead
(circa A.D. 1950...mid 1980)
(known language/author)
 “come across the men” --> prob = 0
85
I have to go to the store.  I need butter.
The chair’s leg is broken
 that learns a classifier to assign to unseen sentences one of a fixed
 apply language model to unknown
Carter told Mubarak he shouldn’t run again.Carter told Mubarak he shouldn’t run again.
35
 58
Stages of NL Understanding
dependencies.
 P(world|BANK1) = (1+.5) / 55  P(world|BANK2) = (0+.5) / 37
target word [0, 0, 0, 2, 1, 0, …]
John.
86
= automatic processing of written texts
 I went to the bank of Montreal and deposited 50$.
 training on cooking recipes and testing on aircraft
Lexical Semantics
apples 1
 sense of a word depends on the sense of surrounding words
P(I want to eat British food)
 constituents & parts-of-speech
Slide 67
S3: Eat your soup!
 N-by-N matrix of probabilities/frequencies

Slide 55

84
 Features?
Remember these slides?
of the grammar to a specific sentence.
Naïve Bayes WSD
 AGCTTCG ... A? G? C? T?
 bigram of characters
76
 Input = acoustic signal
Bag-of-word Model (BOW)
18
Languages
Mary did like to eat apples.
 P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37
15
98
“One morning I shot an elephant in my pyjamas.  How he got
 depth-first: exhaust 1 path before considering
S1: Translate that!
)P(s kk 
 E.g.: Could you spare some change?
102
23
sequences
 Gun = instrument that can kill
Parsing (Syntax):
a aardvark aardwolf aback … zoophyte zucchini
is goodThe S&P500 jumpedThe S&P500 jumped
 result:
 e.g. preference to shorter rules
 I went to the bank of the river and dangled my feet.
The 13th Shanghai International Film Festival…The 13th Shanghai International Film Festival…
1st Invasion of NLP, from ML
B   N
The waiter ignored us for 20 minutes.The waiter ignored us for 20 minutes.
7. John ate ART N
Applications of LM
P(wn|wn-1)
reducing fever in patients with acute
Paraphrase
 Features: words [fishing, big, sound, player, fly, rod, …]
 Fake gun… it is a gun anyways?  Can it kill?
Start:   10:00am
How people use language in a social
 Michael Bublé, who was featured in ..., is living in California.

104
60
.
Slide 43
Slide 17
Deep
the camera feels flimsy, is plastic and very light in weight you have
 P(the|BANK1) = 5/30 P(the|BANK2) = 3/12
25
 VP: ∀i ∑i P(VP --> B) = .7 + .3 = 1
 V is the feature vector
Chinese 2 0 0 0 0 120 1  C(Chinese)=213
individual words in a text
 Natural
  intuition:
“Shannon Game” (Shannon, 1951)
Slide 48
 S --> NP VP
Trigrams
Slide 63
Slide 89
Language model -- P(a sentence)
31
Remember this other slide...
Economy
Semantics: What is the
Do you have a child?
 Using our general knowledge of the world to interpret
Question Answering: IBM’s Watson
 Optical character recognition / Handwriting
 Semantic roles: Agent, Patient, Instrument, Time,  Location, …
 Water gun… a gun made out of water?
2. NP VP
Slide 91
linguistic features are hand-engineered and fed to the ML model
 A non-probabilistic parser may find a large set of possible
7
 Sentence  Ex. If it rains, I will go out.
Probabilistic Parsing
n-gram Model
aardwolf 0 0 0 0 … 0 0
 P(“I’d like a coffee with 2 sugars and milk”) ≈ 0.001
ELABORATION The solution was developed by Alan Turing.
 Encode information about the words around the target word
The Ancient Land of NLP (aka GOFAI)
Party
Size and weight
Slide 12
…he could not take it anymore.
Surrounding words Most probable sense
B: number of "bins"
 Happy [cats and dogs] live on the farm
75
Language
 parsing:
= .0009072         = .0006804
Chair:  person?  Furniture?
  How parts-of-speech are organised into larger
5. PN V ART N
 map an expression into a knowledge representation
Mary did not like to kill
The Dow Jones is upThe Dow Jones is up
)s|P(v

Colorless   green   ideas   sleep   furiously.Colorless   green   ideas   sleep   furiously.
Why use only bi- or tri-grams?
Slide 10
Slide 64
tree) to train a system
Probability of the candidate
 Input = text
K-means clustering
e.g. for bigrams, it's (size of the vocabulary)2
Slide 13
you like (love) your mother.
 “the” appears many more times, than “rabbit”
(5) NP -- > Det N
 Value: frequency of these words in a window before & after the
(2) S --> VP
Slide 20
S)P(S)|P(OargmaxS*
as well as it understands your
54
human
ceword sequenc
Syntax
Stoker?
37
 21
• Engineering Approach
 Ex: “Astronomers saw stars with ears.”
food 20 1 18 1 1 1 1  C(food) + |V| = 3122
 Red Tape Holds Up New Bridges
1. Assign the right part of speech (NOUN, VERB, …) to
Language Identification, Text Classification, Authorship Attribution...
 The model assigns a probability of zero to unseen events …
NLP vs Speech Processing
 The bird next to the large oak tree near … flies rapidly.
 E.g.:
 trigrams (characters) after some billions of words
Pragmatics
89
Fully automatic
 Disambiguation: “I lost my left shoe on the banks of the river Nile.”
Slide 34
Slide 50
第 13 届上海国际电影节开幕…第 13 届上海国际电影节开幕…
Slide 42

1
 a tree representation of the application
 sentence S
features
 Let C(w1...wn) be the frequency of n-gram w1...wn
Slide 15
Slide 31
 Decide on training corpus
t  w
 Number of bigrams: 314,843,401
Slide 104
94
Mary 2
 we need:
 …
 Michael Jackson, who was featured in ..., is buried in California.
 next base pair based on past DNA sequence
 Humans infer relations between sentences that may not
Spam detection
Summary of NLU
Slide 1
 Statistical Machine Translation
Mary did kill John.
argmax
1. John ate the cat
other
k
eat 0 0 2 0 19 2 52  C(eat)=938
Slide 23
Slide 95
 use machine learning techniques (ex. Naïve Bayes classifier, decision
 word order is ignored ==> meaning of text is lost.
Statistical methods / Machine Learning / Knowledge-poor method
 how the syntactic analysis are to be computed
P(word sequence | acoustic signal)argmax
 description of the language constructions
 I ate spaghetti with a fork.  <instrument>
Slide 16
 An electric guitar and bass player stand off to one side,
…Web site of the European Central Bank/BANK1 located in Frankfurt…
Slide 6
 Metal gun… a gun made out of metal
 so the probability of a word also depends on the
 Training phase:
 More complex models of language are needed to handle such
 Prepositional phrase attachment (PP-attachment)
Slide 29
 2
Feature Extraction
 “pill”?  “broccoli”?
 parsing is seen as a search problem
phonemes in the language +
 at any given point, “the”  is more probable than “rabbit”
(designed by hand)
1. The computer understands you
 left right right up ... up? down? left? right?
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554
patte
 Input:
kt
1s
 construction of the language
 Is/VERB red/ADJ the/DET rose/NOUN.
8P(II)
slide from Olga Veksler (U. Western Ontario)
P(S)S)|P(O argmaxS*
 instead of adding 1, add some other (smaller) positive value 𝛿
70
through the space of all possible parse
(4) NP --> Det N PP
Slide 45
 “Get the cat with the gloves”
unsmoothed bigram conditional probabilities:
I 8   9 1087
or
26
Bottom up   
 Target word: The word to be disambiguated
 NB spam filter seen in class a few weeks ago
Metropolis
2. Train a character-based language model for Spanish:
-- > (Maddy, my dog), and (Samy)
 where:
90
n-grams
Nb of occurrences of feature j
1. Natural Language Understanding
n
Mary did not like to eat
Slide 57
)...wC(w  )...ww|(wP
 Compositional
the cat eats the mouse.
Another Example of Ambiguity
Slide 30
-Chris
Slide 81
(9/5053)
Slide 90
i.e. nb of different n-gram types
 Ban on Nude Dancing on Governor’s Desk
1. Introduction
72
74
Stages of NLU
P(acoustic signal | word sequence) x P(word sequence)argmax
...
The trophy would not fit in the brown suitcase because ...
 Goal: find most likely sentence (S*) given the observed sound (O) …
 to assign semantic roles (different from grammatical roles):
 I ate spaghetti with my sister.     <accompanying person>
Slide 62
Slide 71
 What if a sequence never appears in training corpus? P(X)=0
dinner party, Friday May
46
 a non-terminal designated as the starting symbol
4. Linguistic features for NLP
N: size of the corpus
I need new batteries for my mouse.I need new batteries for my mouse.
 next word based on past words
42
P(“che bella cosa”) with the Spanish LM
In Speech Recognition
100
 Ex: Silence of the lambs…
P(“che bella cosa”) with the Italian LM
want .0014 .00035 .278 .00035 .0025 .0031 .00247
= estimating P(vj|sk) and P(sk) from a sense-tagged training
Where we are today
food 19 0 17 0 0 0 0  C(food)=1506
 Solution: smoothing
Slide 36
May
the  cat Prep-NP
 but there are so many of them that too much probability mass is
 World knowledge
Statistical NLP
1st approximation
 ex. the, in, and, over, beyond…
 Open (lexical) class words
16
Slide 51
Information Extraction & Sentiment Analysis
.215 .00019 .0028
NP-VP       VP   Aux-NP-VP
 5-grams
Summarization
B  N
Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014
Syntactic
82
 The meaning/sense of words is not clear-cut
8
S
101
 E.g.
 with 100,000 words, the probability of each word at any
i.e. nb of cells in the matrix
Translation from Stanford’s Phrasal:
Add-one: Example
The professor sent the student to see the principal because…
 The men next to the large oak tree near … are tall.
 Local HS Dropouts Cut in Half
Problem: Multiple parses
…he wanted to see him.
Best roast chicken in San Francisco!Best roast chicken in San Francisco!
 Quantifier Scoping
 Connectives such as “if”, “however”, “in conclusion”
 in a training corpus, we have 10 instances of
2nd Invasion of NLP, by Deep Learning
 having a single non-terminal on the LHS and one or more
 E.g. Overlapping of word senses across languages
 John visited Paris. Then he flew home. He went to
 Knowing that Sue “swallowed” helps narrow down
the next event to occur in the sequence
 e.g. first-order predicate logic, conceptual graph, embedding...
 Teacher Strikes Idle Kids
sentence in the language
P(acoustic signal | word sequence) x P(word sequence)
…for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...
 Intuitively:
9. John ate the cat
affordability
aback 26 1 6 0 … 12 2
A 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014
8I)|P(I
 NP --> D N
nice and compact to carry!
Date:   Jan-16-2012
 He is trying to find out.
 P(more | come across) = 0.1
I want to eat Chinese food lunch … Total
 “come across 3 men” --> prob = 0
Word Sense Disambiguation (WSD)
Hi Dan, we’ve now scheduled the curriculum meeting.
Machine Translation
 P(a | come across) = 0.1
 Take a window of n words around the target word
87
 Assume V = 50
ABC has been taken over by XYZABC has been taken over by XYZ
for long sentences
mostly solved making good progress Good progress by
 What words are available in a
 S, NP, VP, PP, D, N, V, ...
…play… instrument
 Markov approximation is still costly
Introduction to
 “mountain”? “tree”?
Example: Language Identification
What’s a Language Model?
 Number of fivegrams: 1,176,470,663
→ Worksheet #10 (“Parsing”)
(6) PP -- > Prep N
 VP: verb phrase       eat an apple, sing, leave Paris in the night
Current Research area: see Winograd Schema Challenge
1 14 1 1 1  3437
Slide 100
since the camera is small and light, I won't need to carry around
 P(world|BANK1) = 1/30 P(world|BANK2) = 0/12
to be very delicate in the handling of this camera
previously unseen events
“At last, a computer that understands you like your mother”
 breadth-first: compute all paths in parallel
Slide 73
 Discourse Analysis
 (I eat) (eat I) (I sleep)
into my pyjamas, I don’t know.”
 Number of unigrams: 13,588,391
 3
perfume.  Then he flew home.  He went to Walmart.  He
to see in a few classes
as) across C(come  across) come |P(as 
syntactic constituents
 tries to explain what the speaker is really expressing
 Closed (functional) class words
 Bottom-up parsing /
8. John ate the N
Helping human translators
 Bayes decision rule:
... it was too small.
Slide 92
Create new Calendar entry
 The cat eats the mouse = The mouse is eaten by the cat.
Coreference resolution
Question answering (QA)
 Stolen Painting Found by Tree
1. Train a character-based language model for Italian:
kj
Word sense disambiguation (WSD)
size and weight
 P(X | come across) = 0  where X ≠ “as”, “more”, “a”
3. PN V the cat
not 1
Remember this slide...
CAUSE Because I was sick, I could not do my assignment.
stemming
20
 Speech Recognition
 “Just then, the white …”
Slide 102
 [Happy cats] and dogs live on the farm
Depth
marker, …
Depth-first vs Breadth-first
Slide 22
17
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831
 possible variations: case sensitivity,
Lexicon:
 Output:
2. Natural Language Generation
tagging
…he was throwing paper balls in class.
together?
Why is NLP hard?
34
 second-order Markov models
 Hi dear, how are ... helicopter? laptop? you? magic?
s
 P(“I’d hike a toffee with 2 sugars and silk”) ≈ 0.000000001
Slide 44
over nb of all occurrences of
log(P(British|eat)) + log(P(food|British))
Slide 80
• Rules are developed by hand in collaboration with linguists
 The man saw the boy with the telescope.
 A word may denote different things (ex. chair)
John 1
Slide 7
 A Language model is a n-gram model over word/character
…         total = 10,000
To: Dan Jurafsky
Probability of ≠ pronunciations
 Number of sentences: 95,119,665,584
 i.e. To predict the next event in a sequence of event
Slide 88
Slide 49
kill 1
ADJ         ADJ    NOUN  VERB      ADV     ADJ         ADJ    NOUN  VERB      ADV
understands you.
 but some words are more frequent then others…
 The man next to the large oak tree near … is tall.
character/word sequences
 British Left Waffles on Falkland Islands
 with pre-classified documents
6. NP V ART N
LS
 Every man loves a woman.
Slide 53
Q. How effective is ibuprofen in
48
 --> need to pick the most probable parse one from the set
as well as your mother
19
 Used when the past sequence of events is a good indicator of
 NP: noun phrase      the little boy from Paris, Sam Smith, I,
to any other rules for VP
Example (with add 0.5 smoothing)
92
78
some training corpus
given to unseen events
 Number of tokens: 1,024,908,267,229
I want to eat Chinese food lunch …
1. Lexical Semantics
Slide 28
 using a language model > trigram is impractical
Google’s Web 1T 5-gram model
 Spelling correction
)...wC(w
C 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014
 But has severe limits to understand meaning of text...
 Features?
 … …
bought some underwear.
Linguists
… … … … … … … …
 And a total of N = 10,000 bigrams (~word instances) in the training corpus
sentences?
 We can use Bayes rule to rewrite this as:
 Juvenile Court to Try Shooting Defendant
27
The Modern Land of
E 0.0097 0.0014 0.0014 0.0014 … 0.0014 0.0014
ambiguous word
Deep Language Processing
 Very ambiguous
-- > (Maddy), (my dog), and (Samy)
Slide 79
Enter Source Text:
4. PN V ART cat
Deep Learning
Heuristic
 Conjunctions and appositives
49
zoophyte 0 0 0 1 … 0 0
3. Given a unknown sentence “che bella cosa”  is it in Italian or in Spanish?
 but does not take word order into account.  This
1-n1n 
4. Highest probability  language of sentence→
 cat, mouse, nurses, eat, ...
…violin… instrument
 a representation of context-independent, literal meaning
 Syntactic dependencies
 Output = text
Hide (agent=child, patient=candy,
 so instead of multiplying the probs, we add the
WALLACHIA AND MOLDOVIA”
29
eat .00039 .00039 .0009 .00039 .0078 .0012 .0208
 Testing phase:
• But the linguistic features are hand-engineered and fed to the ML model
I 8 1087 0 13 0 0 0  C(I)=3437
 goal:
Word Freq.
 Training a Naïve Bayes classifier
https://en.wikipedia.org/wiki/Winograd_Schema_Challenge
Slide 59
CS &
the rose is red /  red the rose is
 Grammatical roles:  subject, direct object, ...
Slide 24
Slide 74
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html
 (?) I am going to Concordia.  I need butter.
 every previously unseen n-gram is given a low probability
 “Sue swallowed the large green ______ .”
Artificial Intelligence:
2. Bag of word model
 I eat.  I sleep.
to 1
 Many possible parses for a single sentence happens

across) C(come
Grammar:
Y 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014
Mary did eat apples.
 See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
a 0 0 0 0 … 8 5
 Semantic dependencies
Information Extraction
parses
events (grams/units/items)
 But, how far back do we look?
 Heuristic search:
S2: Translated this!
WSD as a Classification Problem
 i.e., going back 3 words before helps
 product of probabilities… numerical underflow
 a set of re-write rules
eat 1
Correct parse 1 Correct parse 2
 each word has an equal probability to follow any
Add-one: Example (con’t)
given point is .00001
flash
 Hospitals are Sued by 7 Foot Doctors
space to unseen events
Probability of a parse tree
  Input:
add-one bigram conditional probabilities:
N=10,000
 The child hid the candy under the bed.
 …
 24 GB compressed
Slide 39
source:  Manning, and Schütze, Foundations of Statistical Natural Language Processing, MIT Press (1999)
 So use a window of words around the target word as
Slide 98
before considering another
Slide 19
Sentiment analysis
 so we have:
t
47
5. John V NP
You’re invited to our
1  )w w (w C  )w w (wP n1 2n21Add1
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) +
“ing”  is more probable than in
 ...
I have to go to the university.  I need butter.
 Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) …
 I ate spaghetti with lots of appetite. <manner>
 He is trying to fine out.
Menu
French
 a parsing strategy:
text
 hypothesis: texts that resemble
P(acoustic signal)
1. Speech Recognition
 Won Jeopardy on February 16, 2011!
G. Marx, Animal Crackers, 1930.
 I ate spaghetti with meat balls.  <attribute of food>
Artificial Intelligence:  Natural Language Processing

33
 understanding how people use language socially
Slide 37
 Size:
Information
breadth first
correct sentences
problem
appearing in windows of Sk
 P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37
meaning/semantic
 Lexical Semantics :
 models the order of the events
apples.
 Assume we translate from fr[foreign] to English  i.e.: (en|fr)
Retrieval
facts, …) modifies our understanding of
leg
Some Difficulties
27 at 8:30
 n-grams take [a bit of] word order into account
cewordsequen
How knowledge about the world (history,
Natural Language Processing
 Goal:
 Information Retrieval (eg. google search)
 the probability of expanding VP by a V NP, as opposed
This is only a matter of time.
en
2. Count words and build model
unsmoothed bigram counts:
English Parts-of-Speech
individual words?
Machine Learning
 We painted the wall with cracks.
36
The man saw the boy with the telescope.
…         ...
 Sentences can be very ambiguous…
 Iraqi Head Seeks Arms
 Clean and tokenize
P(wn|wn-1wn-2)
 ie: events = words  or  events = character
Problem with n-grams
63
 NP --> PN
2. Compositional Semantics
over the total nb of features
 Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) …
38
 S is the set of possible senses for the target word
 How to arrange words
MOST FAMOUS NOVEL
 1 time, followed by “a”
not really part of the scene, just as a sort of nod to gringo
 The idea is to give a little bit of the probability
 a grammar:
…striped… fish
Slide 8
WILLIAM WILKINSON’S
language) share similar
 Artificial
Buy V1AGRA …Buy V1AGRA …
 some languages do not have all these categories
Slide 58
A Parse Tree
Find: The most likely English
 ex: bass = fish, musical instrument, ...
 set of non-terminal symbols
32
Slide 66
 24
95
 V = {a, aardvark, aardwolf, aback, … , I, …, want,… to, …, eat, Chinese, …, food, …, lunch, …,
14
1088
Learning
Factors of Training Corpus
 breadth-first:
source: Robert Dale.
 P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12
Slide 65
44
 so that there is a little bit of probability mass left over for
i.e. nb of n-gram tokens in training corpus
S3: …
Slide 25
Slide 27
like 1
 P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)
 (set of) parse trees
Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829
End:    11:30am
51
(mid 1980 – circa 2010)
Find: The most likely word/sentence – S*
 How do we deal with sentence boundaries?
 12
99
n1
 “the large green ______ .”
Slide 18
 Large and open vocabulary (new words everyday)
83
…salmon… fish
 generally function/grammatical words
Slide 78
  Discourse tagging can be viewed as typical classification problem
 8 times, followed by “as”
 ie. pick the sentence with the highest probability:
in a specific sentence
Compositional Semantics
Attributes:
D 0.0042 0.0014 0.0014 0.0014 … 0.0014 0.0014
 ie. The meaning of individual words
= finding the most likely sense k
Slide 38
 NP --> Pro
 Goal: choose the most probable sense s* for a word given a vector
Walmart. He bought Mary some expensive perfume. He
 characters = 26 letters (case insensitive)
trees
 sk is a sense in S
 Syntax is not enough
96
 Complex syntactic structures
Slide 76
 Output = acoustic signal
79
2. Speech Synthesis
animal
 PP: prepositional phrase  in the morning, about my ticket
“come across”
Acoustic model --
Acoustic model
= 2,621,456
 An n-gram model is a probability distribution over sequences of
Slide 94
 first-order Markov models
Slide 84
Deep Neural Networks applied to NLP problems
8. NP VP

 P(and|BANK1) = 1/30 P(and|BANK2) = 0/12
2. Determine how words are put together to form
Slide 14
 Kids Make Nutritious Snacks
 new_count(n-gram) = old_count(n-gram) + 1
add-one smoothed bigram counts:
…
S1: How to recognize speech.  ?
Some Adjustments
febrile illness?
 used in many applications:
✗
Translation model
P(en) x en)|P(fr argmaxen*
45
 probability of an n-gram involving unseen words will be zero!
53
A B C D … Y Z
 In English character sequence
 Number of fourgrams: 1,313,818,354
…Welcome to the Bank/BANK1 of America the nation's leading financial institution…
52
Breath
 Most likely sense of the word
lunch 4 0 0 0 0 1 0  C(lunch)=459
 AdvP: adverb phrase slowly, really slowly
Slide 96
Model Applications
 nouns, main verbs, adjectives, adverbs
→ Worksheet #10 (“Sentence Probability”)
speech
Language model
 most widely used value for 𝛿 = 0.5
 Map sentences to some representation of its
91
Slide 101
40
)count(s
to .00082 .0002 .00226 .1767 .00082 .0002 .00267
corpus
mother.
Slide 61
The Ancient Land of NLP
 use machine learning techniques (ex. Naïve Bayes
Slide 60
Slide 97
 new words can be added easily
Smurf talk on youtube:
https://www.youtube.com/watch?v=7BPx-vl8G00
7. NP V NP
Housing prices roseHousing prices rose
437 3
 NP: ∀i ∑i P(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
 Number of trigrams: 977,069,902
Slide 85
Event:  Curriculum mtg
…Welcome to America's Job Bank/BANK1 Visit our site and…

Forest
 the more, the better
very often…
 Not in NL:
In Statistical Machine Translation
 i.e., Meaning of individual words
 I shot an elephant in my pyjamas.
 Even simple sentences are highly ambiguous
translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310
to 4 1 11 861 4 1 13  C(to) + |V| = 4872
 generated from 1 trillion words
Search
zoom
Using World Knowledge
= automatic processing of speech
a sentence/discourse
 Pretend we have seen every n-gram at least once
(10) Det --> the
 Training corpus (context window = ±3 words):
relations between
Slide 35
Slide 83
 Pragmatics
1. Data preparation:
 Language Identification
Slide 3
• Rules are developed automatically (using machine learning)
 Tense of verb (future, past)
✓
Einstein met with UN officials in PrincetonEinstein met with UN officials in Princeton
13
 Assume a vocabulary of 1616 (different) words
65
meaning of phrases and
4. John VP
Slide 11
1-n1
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
XYZ acquired ABC yesterdayXYZ acquired ABC yesterday
John did not kill Mary.
location=under_the_bed, time=past)
maintenance manuals
93
2. The computer understands that
Machine
http://en.wikipedia.org/wiki/Google_Ngram_Viewer
purpose, …)
parsing
 e.g., logics, semantic network, embedding…
https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-
punctuation, beginning/end of sentence
Rule-based NLP
 bottom-up (data-directed): words --> grammar
Turing was a great mathematician living in
Slide 4
Neural networks
S4…
88
)s,count(v
 Because it is ambiguous:
Examples of Discourse Relations
Part-of-
each other (same author, same
Speech
Date: January 15, 2012
 s* = argmaxsk P(sk|V)
Add-one Smoothing
étape
Island
Example: John ate the cat
… … … … … … … 0.0014
sentence – en*
text?
(Dracula)
Castro Theatre at 7:30.
unseen examples one of a fixed number of senses
)count(word
Slide 5
chair
(11) N --> cat
)w w (w C  )w w (wP n1 2n21AddD
aardvark 0 0 0 0 … 0 0
22
 Not tolerant to errors (ex. Syntax error)
…
Great Britain. He was an atheist as well as gay.
  Main Constituents:
77
surrounding sentences?
  These phenomena can quickly increase the number of
 The man next to the large oak tree near … talks rapidly.
PP attachment:
Slide 46
INSPIRED THIS AUTHOR’S
 …
O)|P(SargmaxS*
Applications
(1) S --> NP VP
 there may be long-distance dependencies.
Bill Gates passed away last night.
 S: sentence       The boy is happy.
jambe pied
 set of terminal symbols
Slide 93
is the bag of word approach.
number of discourse relations (categories)
World Knowledge
Top down   
How to relate the meaning of sentences to
Slide 86
It will be in Gates 159 tomorrow from 10:00-11:30.
 Speech Processing
 but after a while, not much improvement…
…lounging against verdant banks/BANK2 carving out the...

Slide 54
 next move of player based on his/her past moves
55
11
 Smaller vocabulary
Given: Foreign sentence - fr
Do you have a quarter?
argmax:
https://en.wikipedia.org/wiki/Claude_Shannon
 N-by-N-by-N matrix of probabilities/frequencies
What is the
log of the probs
 P(BANK1) = 5/7 P(BANK2) = 2/7
 |V| = 1616 words
66
1. S
 (<s> I) (I eat) (eat <s>) (<s> I) (I sleep) (sleep <s>)
 compute 1 level at a time
zoophyte, zucchini}
)s|P(v log  )P(s logargmaxs*
Slide 56
model
And Even More Examples of Ambiguity
C(I) + |V| = 5053

...
这 不过 是 一 个 时间 的 问题 .
is good
Slide 87
a CFG consists of
68
SF?
journey
https://www.youtube.com/watch?v=sbJ89LFheTs
Summary of Parsing Strategies
zucchini 0 0 0 3 … 0 0
Automatic Language Identification…
 bigram needs to store 400 million parameters
56
Slide 32
 Genre (adaptation):

50
 Predict the next word/character given the n-1 previous
B 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014
 assign syntactic structures to a sentence
ease of use
BANK1 BANK2
• Applications: Information Retrieval, Predictive Text / Word Completion,
 Heuristic search
lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075
…The Asian Development Bank/BANK1 ADB a multilateral development finance…
 Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) …
(categories)
…Today the World Bank/BANK1 and partners are calling for greater relief…
 Parallel problems to syntactic ambiguity
possible parse trees!
of  Deep
 AdjP: adjective phrase    really funny, rather clear
tokenisation Decision trees
.00019 .00019 .00019
guess how that’s done?
Subject: curriculum meeting
3. The computer understands you
3. PN VP
39
(12) VB --> eats
classifier, decision tree) to train a system
6. John ate NP
 words & punctuation
Let’s go to Agra!Let’s go to Agra!
Nb of  occurrences of sense k
PERSON              ORG                      LOCPERSON              ORG                      LOC
Do you want a ticket?
previous words (the history)
City of
 Since denominator is the same for each candidate S, we can ignore it for the
 See discussion:
Parsing Strategies

Problem: Data Sparseness
language?  gfiioudd  / table
Slide 82
Building n-gram Models
words/characters.
 depth-first: exhaust 1 path
Information extraction (IE)
 So for:
 E.g.: figures of speech, …
Slide 47
Slide 99
“AN ACCOUNT OF THE PRINCIPALITIES OF
Processing
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)
 1 time, followed by “more”
…
59
 bigrams (characters) after 100’s million words
 aka stop words
I can see Alcatraz from the window!I can see Alcatraz from the window!

 P(off|BANK1) = 0/30 P(off|BANK2) = 1/12

P(wn |w1w2…wn-1)
Syntactic Parsing
N+|V|2 = 10,000 + (1616)2
meaning
62
000 10
 General Kane… person     but  General Motors … corporation
 Simple syntactic structures
 In logics:
 Non-ambiguous
Slide 26
 Natural language is not linear ....
Part-of-speech (POS) tagging
4
Bigrams
unsmoothed bigram counts (frequencies):
Slide 77
terminal or non-terminal in the RHS
... it was too big.
Slide 33
Another Classification Problem,  again!
Limits of BOW Model
want 3 0 786 0 6 8 6  C(want)=1215
First
 WSD can be viewed as typical classification
d
Dialog  Where is Citizen Kane playing in
PURPOSE To use the computer, get an access code.
 N = size of the vocabulary we are using
…player… instrument
 Most likely relation in the sentence (none, condition, contrast,
Slide 9
 P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37
Slide 21
Slide 68
Linguistic features used for what?
Semantic interpretation:
10
Named entity recognition (NER)
I .0018
expectations perhaps.
2nd approximation: unigrams
those heavy, bulky professional cameras either!
73
(3) S --> Aux NP VP
 Maybe we should take word order into account...
Slide 70
LS
Who is Bram
 Maddy, my dog, and Samy
 Determining which sense of a word is used
3. n-gram models
Features for WSD
kjk
 A simple model where word order is ignored
The Land of Statistical NLP
Example of a PCFG
3rd word
to 3 0 10 860 3 0 12  C(to)=3256
Semantic Interpretation
possibilities
 The/DET rose/NOUN is/VERB red/ADJ.
 Top-down parsing /
CONDITION  If it rains, I will go out.
…river… fish
• And the linguistic features are found automatically!
2. PN ate the cat
Discourse Analysis
another
recognition
P(O)
• Cognitive approach
2nd word
 Natural Language Processing
 Intuitively, P(VP → V NP) is:
 Training:
Parsing
64
 World Knowledge
 take into account the frequency of the word in
 relatively fixed membership
Given: Observed sound - O
43
SEQUENCE Do this, then do that.
67
 trigram needs to store 8 trillion parameters
6
 goes beyond the literal meaning of a sentence
→ Worksheet #10 (“Word Sense Disambiguation”)
did 2
 decrease the probability of previously seen events
Slide 52
Village of
Machine translation (MT)
be explicitly stated in order to make a text coherent.
Symbolic methods / Linguistic approach / Knowledge-rich approach
 that learns a classifier (a function f) to assign to
