
Economy
corpus
ï® Spelling correction
ï® take into account the frequency of the word in
The Land of Statistical NLP
The Ancient Land of NLP
ï® so instead of multiplying the probs, we add the
of  Deep
ï® Training phase:
ï± a parsing strategy:
â€¦
Slide 74
15
Slide 35
(Dracula)
apples 1
Slide 72
ï® most widely used value for ğ›¿ = 0.5
given point is .00001
ï± P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37
â€œI am going to make a collect â€¦â€
6. John ate NP
Slide 3
linguistic features are hand-engineered and fed to the ML model
Slide 31
eat .00039 .00039 .0009 .00039 .0078 .0012 .0208
6
ï® How to arrange words
the cat eats the mouse.
Example of a PCFG
80
ï® 5-grams
Slide 29
ï±
ï± Metal gunâ€¦ a gun made out of metal
given to unseen events
27 at 8:30
ï® i.e. To predict the next event in a sequence of event
Size and weight
D 0.0042 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
in a specific sentence
recognition
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) +
ï®  Main Constituents:
ï± Even simple sentences are highly ambiguous
17
-Chris
Deep Neural Networks applied to NLP problems
ï± to assign semantic roles (different from grammatical roles):
Slide 30
source:  Manning, and SchÃ¼tze, Foundations of Statistical Natural Language Processing, MIT Press (1999)
ï± Simple syntactic structures
P(â€œche bella cosaâ€) with the Spanish LM
bought some underwear.
Menu
How knowledge about the world (history,
ï‚§ Goal: find most likely sentence (S*) given the observed sound (O) â€¦
n-grams
ï± Number of sentences: 95,119,665,584
51
ï® Artificial
C 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ï± â€œmountainâ€? â€œtreeâ€?
ï± I ate spaghetti with my sister.     <accompanying person>
https://www.youtube.com/watch?v=sbJ89LFheTs
Example
)w w (w C  )w w (wP n1 2n21AddD
ï® Map sentences to some representation of its
ï€½ï‚¼
ï± |V| = 1616 words
ï± that learns a classifier (a function f) to assign to
64
)P(s kk ï€½
95
features
Semantic interpretation:
(3) S --> Aux NP VP
ï± In English character sequence
Mary did like to eat apples.
ï± A non-probabilistic parser may find a large set of possible
ï± 1 time, followed by â€œaâ€
Information extraction (IE)
relations between
ï± relatively fixed membership
Example (with add 0.5 smoothing)
ï® Closed (functional) class words
ELABORATION The solution was developed by Alan Turing.
ï± new words can be added easily
mostly solved making good progress Good progress by
n
ï® bigrams (characters) after 100â€™s million words
dependencies.
â€¦playerâ€¦ instrument
ï± aka stop words
ï± John visited Paris. Then he flew home. He went to
Slide 53
Slide 4
7. John ate ART N
7. NP V NP
ï® I eat.  I sleep.
ï® Encode information about the words around the target word
ï® Michael BublÃ©, who was featured in ..., is living in California.
1. Natural Language Understanding
67
ï± Smaller vocabulary
Paraphrase
zucchini 0 0 0 3 â€¦ 0 0
previous words (the history)
Add-one Smoothing
Chinese 2 0 0 0 0 120 1  C(Chinese)=213
ï± depth-first: exhaust 1 path before considering
ï‚§ ie. pick the sentence with the highest probability:
Island
ï± Most likely sense of the word
ï± General Kaneâ€¦ person     but  General Motors â€¦ corporation
1-n1
2. NP VP
as) across C(come  across) come |P(as ï€½ï€½
Slide 62
Y 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ï€½
13
â€œShannon Gameâ€ (Shannon, 1951)
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html
I have to go to the store.  I need butter.
Top down ïƒ¼ ïƒ¼ ïƒ¼
Semantic Interpretation
ï± NB spam filter seen in class a few weeks ago
82
speech
ï± next word based on past words
appearing in windows of Sk
Slide 57
CONDITION  If it rains, I will go out.
ï± so that there is a little bit of probability mass left over for
ï± Output = acoustic signal
And Even More Examples of Ambiguity
lunch 4 0 0 0 0 1 0  C(lunch)=459
ï® The idea is to give a little bit of the probability
ï® â€œSue swallowed the large green ______ .â€
P(I want to eat British food)
to be very delicate in the handling of this camera
space to unseen events
Stoker?
48
ï® every previously unseen n-gram is given a low probability
87
)s,count(v
City of
ï® each word has an equal probability to follow any
Date:   Jan-16-2012
â€œAt last, a computer that understands you like your motherâ€
Probability of a parse tree
84
ï® ie. The meaning of individual words
B 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
Problem with n-grams
ï® The men next to the large oak tree near â€¦ are tall.
2nd Invasion of NLP, by Deep Learning
ï± The meaning/sense of words is not clear-cut
Parsing Strategies
Part-of-
en
ï± Number of unigrams: 13,588,391
90
PERSON              ORG                      LOCPERSON              ORG                      LOC
understands you.
ï® but does not take word order into account.  This
Slide 46
Slide 86
39
To: Dan Jurafsky
S2: How to wreck a nice beach. ?
1j
100
Correct parse 1 Correct parse 2
56
https://www.youtube.com/watch?v=7BPx-vl8G00
ï® Not in NL:
Slide 12
problem
ï® Training:
37
ï± nouns, main verbs, adjectives, adverbs
Bag-of-word Model (BOW)
ease of use
N=10,000
ï® a non-terminal designated as the starting symbol
Statistical methods / Machine Learning / Knowledge-poor method
Statistical NLP
Acoustic model
1088
ï± next move of player based on his/her past moves
ï® models the order of the events
aback 26 1 6 0 â€¦ 12 2
2. Train a character-based language model for Spanish:
Coreference resolution
ï± Number of tokens: 1,024,908,267,229
ï± new_count(n-gram) = old_count(n-gram) + 1
Limits of BOW Model
Forest
â†’ Worksheet #10 (â€œParsingâ€)
World Knowledge
Slide 51
ï± â€œcome across 3 menâ€ --> prob = 0
19
ï® Genre (adaptation):
ADJ         ADJ    NOUN  VERB      ADV     ADJ         ADJ    NOUN  VERB      ADV
4
sentence â€“ en*
P(wn |w1w2â€¦wn-1)
ï± E.g.: figures of speech, â€¦
ï± P(more | come across) = 0.1
ï± He is trying to fine out.
1. Assign the right part of speech (NOUN, VERB, â€¦) to
75
Where: Gates 159
ï± The child hid the candy under the bed.
Slide 37
The Ancient Land of NLP (aka GOFAI)
6. NP V ART N
ï‚§ Tense of verb (future, past)
ï® Semantic roles: Agent, Patient, Instrument, Time,  Location, â€¦
ï® P(â€œIâ€™d like a coffee with 2 sugars and milkâ€) â‰ˆ 0.001
Slide 89
apples.
Stages of NLU
ï± Non-ambiguous
ï® word order is ignored ==> meaning of text is lost.
unsmoothed bigram counts:
5. John V NP
chair
40
meaning of phrases and
Do you have a quarter?
Lexical Semantics
ï± E.g. Overlapping of word senses across languages
22
ï® So use a window of words around the target word as
Add-one: Example
= .0009072         = .0006804
ï® The model assigns a probability of zero to unseen events â€¦
Slide 16
ï® The bird next to the large oak tree near â€¦ flies rapidly.
Slide 25
Slide 60
Syntactic Parsing
Slide 88
flash
ï® Top-down parsing /
ïƒ¨
eat 1
ï± with pre-classified documents
Semantics: What is the
.215 .00019 .0028
Factors of Training Corpus
Spam detection
ï® Juvenile Court to Try Shooting Defendant
zoom
ï® used in many applications:
8. John ate the N
ï®  These phenomena can quickly increase the number of
ï® Goal:
Example 1:
Symbolic methods / Linguistic approach / Knowledge-rich approach
cewordsequen
Slide 83
Buy V1AGRA â€¦Buy V1AGRA â€¦
Syntactic
ï® E.g.
ï± â€œpillâ€?  â€œbroccoliâ€?
S2: Translated this!
ï® Hospitals are Sued by 7 Foot Doctors
ï® Speech Processing
ï® ex: bass = fish, musical instrument, ...
environment?
ï± map an expression into a knowledge representation
Slide 10
mother.
SF?
sequences
S4â€¦
ï± â€¦ â€¦
WILLIAM WILKINSONâ€™S
ï± Number of bigrams: 314,843,401
â€¦he was throwing paper balls in class.
add-one smoothed bigram counts:
ï± Complex syntactic structures
tagging
86
Q. How effective is ibuprofen in
ï® in a training corpus, we have 10 instances of
before considering another
â€¢ Rules are developed by hand in collaboration with linguists
perfume.  Then he flew home.  He went to Walmart.  He
3. The computer understands you
Slide 17
meaning/semantic
tokenisation Decision trees
P(acoustic signal)
ï® Assume V = 50
another
End:    11:30am
ï® Assume a vocabulary of 1616 (different) words
ï® Syntax is not enough
â€¦playâ€¦ instrument
ï®  Input:
Slide 56
Rule-based NLP
ïƒ·
ï± (?) I am going to Concordia.  I need butter.
ï® Features?
ï± â€œcome across the menâ€ --> prob = 0
ï® We painted the wall with cracks.
ï± E.g.: Could you spare some change?
trees
a 0 0 0 0 â€¦ 8 5
ïƒ¶
ï± top-down (goal-directed): grammar --> words
since the camera is small and light, I won't need to carry around
ï± Clean and tokenize
81
Party
Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014
ï± Conjunctions and appositives
Smurf talk on youtube:
Who is Bram
ï± trigram needs to store 8 trillion parameters
Slide 43
ïƒ¥ï€«ï€½
Remember these slides?
dinner party, Friday May
V of surrounding words
Summary of NLU
42
ï± bottom-up (data-directed): words --> grammar
ï± P(BANK1) = 5/7 P(BANK2) = 2/7
(designed by hand)
Slide 85
Slide 87
â€¦stripedâ€¦ fish
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)
â€¦Today the World Bank/BANK1 and partners are calling for greater reliefâ€¦
ï® Parallel problems to syntactic ambiguity
â€¢ Cognitive approach
ï® In logics:
ï® Sentence  Ex. If it rains, I will go out.
â€¢ Engineering Approach
ï® N = size of the vocabulary we are using
1s
ï± P(X | come across) = 0  where X â‰  â€œasâ€, â€œmoreâ€, â€œaâ€
N+|V|2 = 10,000 + (1616)2
ï® Pretend we have seen every n-gram at least once
1. Data preparation:
Lexicon:
ï® Determining which sense of a word is used
Add-one: Example (conâ€™t)
Slide 27
possibilities
phonemes in the language +
ï® i.e., going back 3 words before helps
Letâ€™s go to Agra!Letâ€™s go to Agra!
Slide 9
P(wn|wn-1)
animal
ï± Ex: Silence of the lambsâ€¦
target word [0, 0, 0, 2, 1, 0, â€¦]
Dialog  Where is Citizen Kane playing in
Slide 77
1st approximation
Mary did kill John.
some training corpus
i.e. nb of different n-gram types
ï® parsing is seen as a search problem
4. Linguistic features for NLP
Heuristic
PP attachment:
104
ï® The cat eats the mouse = The mouse is eaten by the cat.
55
Metropolis
add-one bigram conditional probabilities:
ï® Natural language is not linear ....
102
)...wC(w
WALLACHIA AND MOLDOVIAâ€
Slide 76
you like (love) your mother.
25
Slide 79
ï± possible variations: case sensitivity,
ï± 1 time, followed by â€œmoreâ€
ï® Take a window of n words around the target word
Probability of â‰  pronunciations
ï± e.g., logics, semantic network, embeddingâ€¦
2nd word
nice and compact to carry!
61
Slide 97
ï± sense of a word depends on the sense of surrounding words
kjk
26
ï± generally function/grammatical words
expectations perhaps.
WSD as a Classification Problem
ï± I ate spaghetti with a fork.  <instrument>
Stages of NL Understanding
Bottom up ïƒ¼ ïƒ¼ ïƒ¼
previously unseen events
i.e. nb of cells in the matrix
S1: Translate that!
to any other rules for VP
ï® first-order Markov models
MOST FAMOUS NOVEL
S3: â€¦
ï® Output:
Why is NLP hard?
k
Slide 24
ï± s* = argmaxsk P(sk|V)
individual words?
Slide 7
ï± compute 1 level at a time
Given: Observed sound - O
LS
http://en.wikipedia.org/wiki/Google_Ngram_Viewer
individual words in a text
Machine Learning
Information Extraction
LSïƒ
â€¦         ...
over the total nb of features
.
1
size and weight
4. John VP
like 1
Using World Knowledge
ï€«ï‚¼
leg
ï± Very ambiguous
ïƒ¸
ï± Water gunâ€¦ a gun made out of water?
In Statistical Machine Translation
ï± AdjP: adjective phrase    really funny, rather clear
across) C(come
ï± AdvP: adverb phrase slowly, really slowly
be explicitly stated in order to make a text coherent.
ï‚§ Connectives such as â€œifâ€, â€œhoweverâ€, â€œin conclusionâ€
ï®  How parts-of-speech are organised into larger
ï® a set of re-write rules
Slide 38
Bigrams
kill 1
ï‚§ â€¦
Slide 40
ï® V is the feature vector
Carter told Mubarak he shouldnâ€™t run again.Carter told Mubarak he shouldnâ€™t run again.
ï® n-grams take [a bit of] word order into account
Slide 21
ï® probability of an n-gram involving unseen words will be zero!
è¿™ ä¸è¿‡ æ˜¯ ä¸€ ä¸ª æ—¶é—´ çš„ é—®é¢˜ .
2. Natural Language Generation
â€¦Web site of the European Central Bank/BANK1 located in Frankfurtâ€¦
Find: The most likely English
Slide 39
Slide 61
Slide 81
4. Highest probability  language of sentenceâ†’
...
45
18
â€œingâ€  is more probable than in
ï± result:
(circa A.D. 1950...mid 1980)
Nb of  occurrences of sense k
ï± P(and|BANK1) = 1/30 P(and|BANK2) = 0/12
91
not really part of the scene, just as a sort of nod to gringo
ï± using a language model > trigram is impractical
ï± goal:
English Parts-of-Speech
96
Slide 66
36
8I)|P(I
French
â€¦violinâ€¦ instrument
https://en.wikipedia.org/wiki/Claude_Shannon
What is the
ï® depth-first: exhaust 1 path
Attributes:
Slide 34
ï® Feature vector V contains:
Deep
ï® we need:
character/word sequences
71
ï® Goal: choose the most probable sense s* for a word given a vector
Slide 75
Slide 19
ï® What words are available in a
Where is Citizen Kane playing in
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
ï® A Language model is a n-gram model over word/character
as well as your mother
10
ï® Assume we translate from fr[foreign] to English  i.e.: (en|fr)
Natural Language Processing
other
ï® Intuitively, P(VP â†’ V NP) is:
Mary did not like to eat
(6) PP -- > Prep N
â€¢ Rules are developed automatically (using machine learning)
2. Count words and build model
together?
of the grammar to a specific sentence.
Hi Dan, weâ€™ve now scheduled the curriculum meeting.
Slide 15
Artificial Intelligence:
ï± at any given point, â€œtheâ€  is more probable than â€œrabbitâ€
language) share similar
ï® i.e., Meaning of individual words
ï± An electric guitar and bass player stand off to one side,
Processing
I can see Alcatraz from the window!I can see Alcatraz from the window!
The man saw the boy with the telescope.
â€¦
ï® Red Tape Holds Up New Bridges
ï± NP --> PN
ï® a representation of context-independent, literal meaning
Problem: Data Sparseness
the rose is red /  red the rose is
It will be in Gates 159 tomorrow from 10:00-11:30.
7
)s|P(v
ï± ...
Building n-gram Models
the next event to occur in the sequence
ï® Size:
human
ï® We can use Bayes rule to rewrite this as:
ï± Number of fivegrams: 1,176,470,663
ï± P(world|BANK1) = 1/30 P(world|BANK2) = 0/12
ï® N-by-N-by-N matrix of probabilities/frequencies
ï± Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) â€¦
(5) NP -- > Det N
argmax
each other (same author, same
5. PN V ART N
66
Slide 28
The Dow Jones is upThe Dow Jones is up
ï® 12
2nd approximation: unigrams
Slide 64
ï® See discussion:
ï® Since denominator is the same for each candidate S, we can ignore it for the
ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦
The chairâ€™s leg is broken
meaning
ï® The/DET rose/NOUN is/VERB red/ADJ.
over nb of all occurrences of
ï® â€¦
Colorless   green   ideas   sleep   furiously.Colorless   green   ideas   sleep   furiously.
78
ï® Intuitively:
words/characters.
https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-
Parsing (Syntax):
ï® S is the set of possible senses for the target word
Given: Foreign sentence - fr
ï± P(world|BANK1) = (1+.5) / 55  P(world|BANK2) = (0+.5) / 37
Another Example of Ambiguity
8. NP VP
Slide 94
ï® description of the language constructions
the camera feels flimsy, is plastic and very light in weight you have
Probability of the candidate
Slide 36
Slide 54
Slide 33
S3: Eat your soup!
(circa 2010-today)
= automatic processing of written texts
Some Difficulties
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
Trigrams
Kingdom
reducing fever in patients with acute
ï± next base pair based on past DNA sequence
ï® ie: events = words  or  events = character
argmax:
Castro Theatre at 7:30.
ï± S: sentence       The boy is happy.
(11) N --> cat
52
77
ï® sk is a sense in S
Slide 11
ï® Testing phase:
add
ï‚´ï€½
log of the probs
marker, â€¦
I have to go to the university.  I need butter.
4. PN V ART cat
2. Compositional Semantics
Question answering (QA)
93
Slide 52
INSPIRED THIS AUTHORâ€™S
Feature Extraction
9. S
P(en) x en)|P(fr argmaxen*
(9/5053)
ï± Information Retrieval (eg. google search)
Examples of Discourse Relations
Slide 63
ï® Bayes decision rule:
B   N
ambiguous word
Start:   10:00am
Bill Gates passed away last night.
Det-N-PP Det-N â€¦
Turing was a great mathematician living in
Slide 91
â†’ Worksheet #10 (â€œWord Sense Disambiguationâ€)
Slide 95
ï® Grammatical roles:  subject, direct object, ...
eat 0 0 2 0 19 2 52  C(eat)=938
29
:note
Features for WSD
Slide 18
Machine
How people use language in a social
N: size of the corpus
punctuation, beginning/end of sentence
Slide 93
to 3 0 10 860 3 0 12  C(to)=3256
ï± Happy [cats and dogs] live on the farm
Slide 5
ï® Pragmatics
ï± (I eat) (eat I) (I sleep)
the  cat Prep-NP
ï± â€¦
i.e. nb of n-gram tokens in training corpus
C(I) + |V| = 5053
(10) Det --> the
language?  gfiioudd  / table
n1
ï® set of non-terminal symbols
Do you want a ticket?
Add-delta Smoothing
Probabilistic Parsing
ï± I went to the bank of the river and dangled my feet.
ï® WSD can be viewed as typical classification
.00019 .00019 .00019
Why use only bi- or tri-grams?
NaÃ¯ve Bayes WSD
https://en.wikipedia.org/wiki/Winograd_Schema_Challenge
Surrounding words Most probable sense
ï± He is trying to find out.
â€¦riverâ€¦ fish
ï® Language Identification
Slide 32
Pragmatics
60
Slide 92
Slide 104
Slide 102
ï® a tree representation of the application
ï® P(â€œIâ€™d hike a toffee with 2 sugars and silkâ€) â‰ˆ 0.000000001
Acoustic model --
2. Determine how words are put together to form
Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829
1. Train a character-based language model for Italian:
P(S)S)|P(O argmaxS*
Date: January 15, 2012
ï® Because it is ambiguous:
ï® <s>I eat <s> I sleep <s>
ï® The man next to the large oak tree near â€¦ talks rapidly.
50
Helping human translators
Model Applications
ï® What if a sequence never appears in training corpus? P(X)=0
is good
ï® Is/VERB red/ADJ the/DET rose/NOUN.
Linguistic features used for what?
1  )w w (w C  )w w (wP n1 2n21Add1
ï± Number of trigrams: 977,069,902
Slide 82
â€¦for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...
1. Introduction
translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310
ï± cat, mouse, nurses, eat, ...
Great Britain. He was an atheist as well as gay.
ï± breadth-first: compute all paths in parallel
ï® Using our general knowledge of the world to interpret
ï® Optical character recognition / Handwriting
ï® But, how far back do we look?
ï± NP: noun phrase      the little boy from Paris, Sam Smith, I,
16
ï± Target word: The word to be disambiguated
46
Einstein met with UN officials in PrincetonEinstein met with UN officials in Princeton
Summarization
ï® Maybe we should take word order into account...
ï± V = {a, aardvark, aardwolf, aback, â€¦ , I, â€¦, want,â€¦ to, â€¦, eat, Chinese, â€¦, food, â€¦, lunch, â€¦,
A B C D â€¦ Y Z
surrounding sentences?
ï® â€œthe large green ______ .â€
zoophyte, zucchini}
68
ïƒ¥
unsmoothed bigram conditional probabilities:
model
Grammar:
65
P(wn|wn-1wn-2)
text?
ï® So for:
)...wC(w  )...ww|(wP
CS &
ï® And a total of N = 10,000 bigrams (~word instances) in the training corpus
79
a sentence/discourse
Slide 1
92
Slide 90
ï± Fake gunâ€¦ it is a gun anyways?  Can it kill?
Example: John ate the cat
ï® Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€
ï± P(a | come across) = 0.1
ï® Local HS Dropouts Cut in Half
ï® Knowing that Sue â€œswallowedâ€ helps narrow down
ï± some languages do not have all these categories
Slide 49
John.
11
ï® 58
ï± S, NP, VP, PP, D, N, V, ...
ï® product of probabilitiesâ€¦ numerical underflow
98
âœ“
31
ï± Every man loves a woman.
59
ï± I ate spaghetti with meat balls.  <attribute of food>
ï® Kids Make Nutritious Snacks
NLP vs Speech Processing
Linguists
CONTRAST This is good, but this is better.
ï® Training a NaÃ¯ve Bayes classifier
74
Breath
43
ï® Discourse Analysis
kt
ï® I shot an elephant in my pyjamas.
n-gram Model
The trophy would not fit in the brown suitcase because ...
ï± John visited Paris.  He bought Mary some expensive
Learning
ï± but after a while, not much improvementâ€¦
ï± Syntactic dependencies
ï± NP --> D N
â€œOne morning I shot an elephant in my pyjamas.  How he got
Applications
ï± training on cooking recipes and testing on aircraft
want .0014 .00035 .278 .00035 .0025 .0031 .00247
Chair:  person?  Furniture?
ï± VP: verb phrase       eat an apple, sing, leave Paris in the night
ï® left right right up ... up? down? left? right?
ï± the probability of expanding VP by a V NP, as opposed
ï± P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37
to .00082 .0002 .00226 .1767 .00082 .0002 .00267
-- > (Maddy), (my dog), and (Samy)
ï® that learns a classifier to assign to unseen sentences one of a fixed
ï® AGCTTCG ... A? G? C? T?
not 1
ï® Statistical Machine Translation
In Speech Recognition
1. Speech Recognition
did 2
with a 20 000 word vocabulary:
1-n1n ï€½
3rd word
Problem: Multiple parses
ï± 8 times, followed by â€œasâ€
Slide 50
ï® The man saw the boy with the telescope.
maintenance manuals
ï® Natural Language Processing
Word Freq.
)count(s
a CFG consists of
terminal or non-terminal in the RHS
âœ—
Slide 2
ï± Gun = instrument that can kill
I 8 1087 0 13 0 0 0  C(I)=3437
This is only a matter of time.
ï± Output = text
I need new batteries for my mouse.I need new batteries for my mouse.
62
depth first
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554
â€¦lounging against verdant banks/BANK2 carving out the...
ï® N-by-N matrix of probabilities/frequencies
ï± e.g. preference to shorter rules
)count(word
Probability of the possible
1. John ate the cat
Artificial Intelligence:  Natural Language Processing
ï± prepositions, determiners, pronouns, conjunctions, â€¦
ï± I went to the bank of Montreal and deposited 50$.
Village of
Best roast chicken in San Francisco!Best roast chicken in San Francisco!
ï± S --> NP VP
Named entity recognition (NER)
Word sense disambiguation (WSD)
Slide 80
...
ï® Hi dear, how are ... helicopter? laptop? you? magic?
Mary did eat apples.
Slide 22
Machine Translation
Slide 98
A 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
Slide 68
A Parse Tree
ï® Natural
ï± having a single non-terminal on the LHS and one or more
... it was too big.
Fully automatic
zoophyte 0 0 0 1 â€¦ 0 0
ï± Heuristic search
ï® breadth-first:
Word Sense Disambiguation (WSD)
2. Speech Synthesis
ï± the more, the better
O)|P(SargmaxS*
ï® but some words are more frequent then othersâ€¦

ï± How do we deal with sentence boundaries?
14
ï® set of terminal symbols
SEQUENCE Do this, then do that.
Slide 71
aardvark 0 0 0 0 â€¦ 0 0
3. Given a unknown sentence â€œche bella cosaâ€  is it in Italian or in Spanish?
ï± characters = 26 letters (case insensitive)
Question Answering: IBMâ€™s Watson
ï® so the probability of a word also depends on the
ï® but there are so many of them that too much probability mass is
Slide 73
ï± Input = acoustic signal
8
journey
Hide (agent=child, patient=candy,
Speech
ï® use machine learning techniques (ex. NaÃ¯ve Bayes classifier, decision
ï® parsing:
-- > (Maddy, my dog), and (Samy)
Translation model
P(word sequence | acoustic signal)argmax
â€¦salmonâ€¦ fish
44
ï® Many possible parses for a single sentence happens
P(acoustic signal | word sequence) x P(word sequence)argmax
ï± Large and open vocabulary (new words everyday)
aardwolf 0 0 0 0 â€¦ 0 0
or
ï® hypothesis: texts that resemble
through the space of all possible parse
Depth
Slide 47
ï® E.g.:
ï® A simple model where word order is ignored
ï® Solution: smoothing
ï± sentence S
Slide 23
ï± apply language model to unknown
into my pyjamas, I donâ€™t know.â€
= estimating P(vj|sk) and P(sk) from a sense-tagged training
Slide 59
ï± use machine learning techniques (ex. NaÃ¯ve Bayes
Slide 78
â€¢ But the linguistic features are hand-engineered and fed to the ML model
Some Adjustments
ï± ex. the, in, and, over, beyondâ€¦
ï® Iraqi Head Seeks Arms
Slide 58
jambe pied
ï± P(the|BANK1) = 5/30 P(the|BANK2) = 3/12
ï± a grammar:
3. n-gram models
ï® assign syntactic structures to a sentence
Nb of occurrences of feature j
Slide 20
ï± NP --> Pro
d
possible parse trees!
ï± [Happy cats] and dogs live on the farm
P(O)
ï® Sentences can be very ambiguousâ€¦
want 3 0 786 0 6 8 6  C(want)=1215
ï® better than add-one, but stillâ€¦
correct sentences
Compositional Semantics
Parsing
May
ï± Not tolerant to errors (ex. Syntax error)
ï® But has severe limits to understand meaning of text...
sentence in the language
32
food 20 1 18 1 1 1 1  C(food) + |V| = 3122
ï± World knowledge
â†’ Worksheet #10 (â€œSentence Probabilityâ€)
ï± Value: frequency of these words in a window before & after the
events (grams/units/items)
â€¦Welcome to America's Job Bank/BANK1 Visit our site andâ€¦
Slide 45
syntactic constituents
ï± Features?
â€¢ And the linguistic features are found automatically!
Slide 101
for long sentences
ï® Ban on Nude Dancing on Governorâ€™s Desk
those heavy, bulky professional cameras either!
ï® Won Jeopardy on February 16, 2011!
ï® Maddy, my dog, and Samy
ï® second-order Markov models
Do you have a child?
88
Youâ€™re invited to our
Another Classification Problem,  again!
â€¦
febrile illness?
ï® understanding how people use language socially
E 0.0097 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
The 13th Shanghai International Film Festivalâ€¦The 13th Shanghai International Film Festivalâ€¦
ï® so we have:
= automatic processing of speech
food 19 0 17 0 0 0 0  C(food)=1506
ï® Predict the next word/character given the n-1 previous
The professor sent the student to see the principal becauseâ€¦
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
ï®  intuition:
ï® â€œtheâ€ appears many more times, than â€œrabbitâ€
Information
as well as it understands your
â€¦he could not take it anymore.
Whatâ€™s a Language Model?
70
ï± Let C(w1...wn) be the frequency of n-gram w1...wn
ï± Decide on training corpus
P(acoustic signal | word sequence) x P(word sequence)
P(â€œche bella cosaâ€) with the Italian LM
NaÃ¯ve Bayes classifier
ï® generated from 1 trillion words
23
ï® Humans infer relations between sentences that may not
Slide 8
ï± A word may denote different things (ex. chair)
Slide 55
Add-one, more formally
3. PN VP
is the bag of word approach.
Search
8P(II)
ï± â€œJust then, the white â€¦â€
I want to eat Chinese food lunch â€¦
38
to 4 1 11 861 4 1 13  C(to) + |V| = 4872
54
ï® bigram of characters
ïƒ§
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
ceword sequenc
Slide 65
ï® Training corpus (context window = Â±3 words):
Part-of-speech (POS) tagging
Ã©tape
ï± Number of fourgrams: 1,313,818,354
Automatic Language Identificationâ€¦
John did not kill Mary.
Slide 100
Language
ï± construction of the language
Discourse Analysis
ï® More complex models of language are needed to handle such
73
â€¦Welcome to the Bank/BANK1 of America the nation's leading financial institutionâ€¦
ï® See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
ï± VP: âˆ€i âˆ‘i P(VP --> B) = .7 + .3 = 1
ï® Input:
Mary did not like to kill
ï± Robust (ex. forgot a comma, a wordâ€¦ still OK)
ï± â€œcome across some menâ€ --> prob = 0
ï® Product of the probabilities of the rules used in subtrees
(12) VB --> eats
Where we are today
ï± I ate spaghetti with lots of appetite. <manner>
Slide 96
Translation from Stanfordâ€™s Phrasal:
1st Invasion of NLP, from ML
1. S
ï® instead of adding 1, add some other (smaller) positive value ğ›¿
ï± Prepositional phrase attachment (PP-attachment)
ï® Quantifier Scoping
99
Deep Language Processing
ï® Michael Jackson, who was featured in ..., is buried in California.
Slide 48
Information Extraction & Sentiment Analysis
Machine translation (MT)
(categories)
Languages
Remember this slide...
is goodThe S&P500 jumpedThe S&P500 jumped
)s|P(v log  )P(s logargmaxs*
log(P(British|eat)) + log(P(food|British))
ï± P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)
27
I 8   9 1087
Walmart. He bought Mary some expensive perfume. He
PURPOSE To use the computer, get an access code.
72
9. John ate the cat
ï® i.e., Meaning of combination of words
t
parses
S)P(S)|P(OargmaxS*
53
â€¢ Applications: Information Retrieval, Predictive Text / Word Completion,
unsmoothed bigram counts (frequencies):
ï® Compositional
parsing
stemming
Applications of LM
89
â€œcome acrossâ€

Slide 6
Slide 67
tree) to train a system
(known language/author)
facts, â€¦) modifies our understanding of
85
Googleâ€™s Web 1T 5-gram model
ï® Bottom-up parsing /
John 1
source: Robert Dale.
83
very oftenâ€¦
Slide 42
â€¦The Asian Development Bank/BANK1 ADB a multilateral development financeâ€¦
How to relate the meaning of sentences to
ï± PP: prepositional phrase  in the morning, about my ticket
ï± Input = text
ï® there may be long-distance dependencies.
49
ï± P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37
ï® 2
patte
1. Lexical Semantics
ï® Heuristic search:
ï® 3
I .0018
B  N
â€œAN ACCOUNT OF THE PRINCIPALITIES OF
ï® trigrams (characters) after some billions of words
ï± Semantic dependencies
Sentiment analysis
K-means clustering
ï® Used when the past sequence of events is a good indicator of
CAUSE Because I was sick, I could not do my assignment.
ï® tries to explain what the speaker is really expressing
Slide 84
ïƒ¦
94
ïƒ
(4) NP --> Det N PP
ï± (<s> I) (I eat) (eat <s>) (<s> I) (I sleep) (sleep <s>)
ï® British Left Waffles on Falkland Islands
S1: How to recognize speech.  ?
Introduction to
XYZ acquired ABC yesterdayXYZ acquired ABC yesterday
ï® 21
3. PN V the cat
(1) S --> NP VP
ï® 24
ï® Bush Wins on Budget, but More Lies Ahead
Deep Learning
guess how thatâ€™s done?
ï± Features: words [fishing, big, sound, player, fly, rod, â€¦]
Language Identification, Text Classification, Authorship Attribution...
B: number of "bins"
57
ï® World Knowledge
Slide 44
101
1. The computer understands you
ï± bigram needs to store 400 million parameters
purpose, â€¦)
3. Smooth your model (see later)
(mid 1980 â€“ circa 2010)
Language model -- P(a sentence)
â€¦         total = 10,000
Event:  Curriculum mtg
text
ï®  Discourse tagging can be viewed as typical classification problem
I want to eat Chinese food lunch â€¦ Total
Example: Language Identification
slide from Olga Veksler (U. Western Ontario)
Subject: curriculum meeting
Syntax
Mary 2
s
Enter Source Text:
... it was too small.
ï® Teacher Strikes Idle Kids
S
= 2,621,456
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014
t  w
G. Marx, Animal Crackers, 1930.
ï® Open (lexical) class words
First
63
1 14 1 1 1  3437
ï± with 100,000 words, the probability of each word at any
34
ï± constituents & parts-of-speech
source: Luger (2005)
NP-VP       VP   Aux-NP-VP
â€¦he wanted to see him.
ï± P(off|BANK1) = 0/30 P(off|BANK2) = 1/12
20
76
ï± --> need to pick the most probable parse one from the set
breadth first
location=under_the_bed, time=past)
2. The computer understands that
ï® how the syntactic analysis are to be computed
Find: The most likely word/sentence â€“ S*
ï¤
Depth-first vs Breadth-first
unseen examples one of a fixed number of senses
ï± decrease the probability of previously seen events
Slide 14
000 10
ï± P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12
437 3
ï± NP: âˆ€i âˆ‘i P(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
Slide 99
Summary of Parsing Strategies
ï® Stolen Painting Found by Tree
Create new Calendar entry
The waiter ignored us for 20 minutes.The waiter ignored us for 20 minutes.
The Modern Land of
Neural networks
affordability
35
ï® An n-gram model is a probability distribution over sequences of
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831
ï® 24 GB compressed
= finding the most likely sense k
ABC has been taken over by XYZABC has been taken over by XYZ
e.g. for bigrams, it's (size of the vocabulary)2
BANK1 BANK2
ï® e.g. first-order predicate logic, conceptual graph, embedding...
number of discourse relations (categories)
ï® Ex: â€œAstronomers saw stars with ears.â€
Current Research area: see Winograd Schema Challenge
lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075
sentences?
Slide 70
ï® goes beyond the literal meaning of a sentence
Remember this other slide...
ï® Lexical Semantics :
Retrieval
Slide 26
33
2. Bag of word model
ï® Markov approximation is still costly
ï€«
to 1
ï± words & punctuation
RESULT  Click on the button, the red light will blink.
classifier, decision tree) to train a system
ï± where:
ï® Most likely relation in the sentence (none, condition, contrast,
to see in a few classes
47
ï± â€œGet the cat with the glovesâ€
a aardvark aardwolf aback â€¦ zoophyte zucchini
ï® Speech Recognition
ï® The man next to the large oak tree near â€¦ is tall.
ï® (set of) parse trees
Language model
2. PN ate the cat
Housing prices roseHousing prices rose
(2) S --> VP
Slide 13
kj
