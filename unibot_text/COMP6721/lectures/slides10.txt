
ï± prepositions, determiners, pronouns, conjunctions, â€¦
ï± top-down (goal-directed): grammar --> words
ï± â€œcome across some menâ€ --> prob = 0
Slide 75
Det-N-PP Det-N â€¦
9. S
Add-one, more formally
depth first
Example 1:
CONTRAST This is good, but this is better.
Add-delta Smoothing
Slide 72
71
Slide 2
S2: How to wreck a nice beach. ?
ï® Product of the probabilities of the rules used in subtrees
Example
Where is Citizen Kane playing in
Kingdom
RESULT  Click on the button, the red light will blink.
80
ï€«ï‚¼
add
â€œI am going to make a collect â€¦â€
ï€«
57
1j
Slide 40
source: Luger (2005)
NaÃ¯ve Bayes classifier
ï® better than add-one, but stillâ€¦
ï± John visited Paris.  He bought Mary some expensive
61
Probability of the possible
Where: Gates 159
with a 20 000 word vocabulary:
ï± Robust (ex. forgot a comma, a wordâ€¦ still OK)
ï® <s>I eat <s> I sleep <s>
ï® i.e., Meaning of combination of words
V of surrounding words
environment?
ï® Feature vector V contains:
81
:note
(circa 2010-today)
ïƒ
3. Smooth your model (see later)
ï® Bush Wins on Budget, but More Lies Ahead
(circa A.D. 1950...mid 1980)
(known language/author)
ï± â€œcome across the menâ€ --> prob = 0
85
I have to go to the store.  I need butter.
The chairâ€™s leg is broken
ï® that learns a classifier to assign to unseen sentences one of a fixed
ï± apply language model to unknown
Carter told Mubarak he shouldnâ€™t run again.Carter told Mubarak he shouldnâ€™t run again.
35
ï® 58
Stages of NL Understanding
dependencies.
ï± P(world|BANK1) = (1+.5) / 55  P(world|BANK2) = (0+.5) / 37
target word [0, 0, 0, 2, 1, 0, â€¦]
John.
86
= automatic processing of written texts
ï± I went to the bank of Montreal and deposited 50$.
ï± training on cooking recipes and testing on aircraft
Lexical Semantics
apples 1
ï± sense of a word depends on the sense of surrounding words
P(I want to eat British food)
ï± constituents & parts-of-speech
Slide 67
S3: Eat your soup!
ï® N-by-N matrix of probabilities/frequencies
ï±
Slide 55

84
ï® Features?
Remember these slides?
of the grammar to a specific sentence.
NaÃ¯ve Bayes WSD
ï® AGCTTCG ... A? G? C? T?
ï® bigram of characters
76
ï± Input = acoustic signal
Bag-of-word Model (BOW)
18
Languages
Mary did like to eat apples.
ï± P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37
15
98
â€œOne morning I shot an elephant in my pyjamas.  How he got
ï± depth-first: exhaust 1 path before considering
S1: Translate that!
)P(s kk ï€½
ï± E.g.: Could you spare some change?
102
23
sequences
ï± Gun = instrument that can kill
Parsing (Syntax):
a aardvark aardwolf aback â€¦ zoophyte zucchini
is goodThe S&P500 jumpedThe S&P500 jumped
ï± result:
ï± e.g. preference to shorter rules
ï± I went to the bank of the river and dangled my feet.
The 13th Shanghai International Film Festivalâ€¦The 13th Shanghai International Film Festivalâ€¦
1st Invasion of NLP, from ML
B   N
The waiter ignored us for 20 minutes.The waiter ignored us for 20 minutes.
7. John ate ART N
Applications of LM
P(wn|wn-1)
reducing fever in patients with acute
Paraphrase
ï± Features: words [fishing, big, sound, player, fly, rod, â€¦]
ï± Fake gunâ€¦ it is a gun anyways?  Can it kill?
Start:   10:00am
How people use language in a social
ï® Michael BublÃ©, who was featured in ..., is living in California.
ïƒ¶
104
60
.
Slide 43
Slide 17
Deep
the camera feels flimsy, is plastic and very light in weight you have
ï± P(the|BANK1) = 5/30 P(the|BANK2) = 3/12
25
ï± VP: âˆ€i âˆ‘i P(VP --> B) = .7 + .3 = 1
ï® V is the feature vector
Chinese 2 0 0 0 0 120 1  C(Chinese)=213
individual words in a text
ï® Natural
ï®  intuition:
â€œShannon Gameâ€ (Shannon, 1951)
Slide 48
ï± S --> NP VP
Trigrams
Slide 63
Slide 89
Language model -- P(a sentence)
31
Remember this other slide...
Economy
Semantics: What is the
Do you have a child?
ï® Using our general knowledge of the world to interpret
Question Answering: IBMâ€™s Watson
ï® Optical character recognition / Handwriting
ï® Semantic roles: Agent, Patient, Instrument, Time,  Location, â€¦
ï± Water gunâ€¦ a gun made out of water?
2. NP VP
Slide 91
linguistic features are hand-engineered and fed to the ML model
ï± A non-probabilistic parser may find a large set of possible
7
ï® Sentence  Ex. If it rains, I will go out.
Probabilistic Parsing
n-gram Model
aardwolf 0 0 0 0 â€¦ 0 0
ï® P(â€œIâ€™d like a coffee with 2 sugars and milkâ€) â‰ˆ 0.001
ELABORATION The solution was developed by Alan Turing.
ï® Encode information about the words around the target word
The Ancient Land of NLP (aka GOFAI)
Party
Size and weight
Slide 12
â€¦he could not take it anymore.
Surrounding words Most probable sense
B: number of "bins"
ï± Happy [cats and dogs] live on the farm
75
Language
ï® parsing:
= .0009072         = .0006804
Chair:  person?  Furniture?
ï®  How parts-of-speech are organised into larger
5. PN V ART N
ï± map an expression into a knowledge representation
Mary did not like to kill
The Dow Jones is upThe Dow Jones is up
)s|P(v
ïƒ§
Colorless   green   ideas   sleep   furiously.Colorless   green   ideas   sleep   furiously.
Why use only bi- or tri-grams?
Slide 10
Slide 64
tree) to train a system
Probability of the candidate
ï± Input = text
K-means clustering
e.g. for bigrams, it's (size of the vocabulary)2
Slide 13
you like (love) your mother.
ï® â€œtheâ€ appears many more times, than â€œrabbitâ€
(5) NP -- > Det N
ï± Value: frequency of these words in a window before & after the
(2) S --> VP
Slide 20
S)P(S)|P(OargmaxS*
as well as it understands your
54
human
ceword sequenc
Syntax
Stoker?
37
ï® 21
â€¢ Engineering Approach
ï® Ex: â€œAstronomers saw stars with ears.â€
food 20 1 18 1 1 1 1  C(food) + |V| = 3122
ï® Red Tape Holds Up New Bridges
1. Assign the right part of speech (NOUN, VERB, â€¦) to
Language Identification, Text Classification, Authorship Attribution...
ï® The model assigns a probability of zero to unseen events â€¦
NLP vs Speech Processing
ï® The bird next to the large oak tree near â€¦ flies rapidly.
ï® E.g.:
ï® trigrams (characters) after some billions of words
Pragmatics
89
Fully automatic
ï® Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€
Slide 34
Slide 50
ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦
Slide 42
ï€½ï‚¼
1
ï® a tree representation of the application
ï± sentence S
features
ï± Let C(w1...wn) be the frequency of n-gram w1...wn
Slide 15
Slide 31
ï± Decide on training corpus
t  w
ï± Number of bigrams: 314,843,401
Slide 104
94
Mary 2
ï® we need:
ï± â€¦
ï® Michael Jackson, who was featured in ..., is buried in California.
ï± next base pair based on past DNA sequence
ï® Humans infer relations between sentences that may not
Spam detection
Summary of NLU
Slide 1
ï® Statistical Machine Translation
Mary did kill John.
argmax
1. John ate the cat
other
k
eat 0 0 2 0 19 2 52  C(eat)=938
Slide 23
Slide 95
ï® use machine learning techniques (ex. NaÃ¯ve Bayes classifier, decision
ï® word order is ignored ==> meaning of text is lost.
Statistical methods / Machine Learning / Knowledge-poor method
ï® how the syntactic analysis are to be computed
P(word sequence | acoustic signal)argmax
ï® description of the language constructions
ï± I ate spaghetti with a fork.  <instrument>
Slide 16
ï± An electric guitar and bass player stand off to one side,
â€¦Web site of the European Central Bank/BANK1 located in Frankfurtâ€¦
Slide 6
ï± Metal gunâ€¦ a gun made out of metal
ï® so the probability of a word also depends on the
ï® Training phase:
ï® More complex models of language are needed to handle such
ï± Prepositional phrase attachment (PP-attachment)
Slide 29
ï® 2
Feature Extraction
ï± â€œpillâ€?  â€œbroccoliâ€?
ï® parsing is seen as a search problem
phonemes in the language +
ï± at any given point, â€œtheâ€  is more probable than â€œrabbitâ€
(designed by hand)
1. The computer understands you
ï® left right right up ... up? down? left? right?
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554
patte
ï® Input:
kt
1s
ï± construction of the language
ï® Is/VERB red/ADJ the/DET rose/NOUN.
8P(II)
slide from Olga Veksler (U. Western Ontario)
P(S)S)|P(O argmaxS*
ï® instead of adding 1, add some other (smaller) positive value ğ›¿
70
through the space of all possible parse
(4) NP --> Det N PP
Slide 45
ï± â€œGet the cat with the glovesâ€
unsmoothed bigram conditional probabilities:
I 8   9 1087
or
26
Bottom up ïƒ¼ ïƒ¼ ïƒ¼
ï± Target word: The word to be disambiguated
ï± NB spam filter seen in class a few weeks ago
Metropolis
2. Train a character-based language model for Spanish:
-- > (Maddy, my dog), and (Samy)
ï± where:
90
n-grams
Nb of occurrences of feature j
1. Natural Language Understanding
n
Mary did not like to eat
Slide 57
)...wC(w  )...ww|(wP
ï® Compositional
the cat eats the mouse.
Another Example of Ambiguity
Slide 30
-Chris
Slide 81
(9/5053)
Slide 90
i.e. nb of different n-gram types
ï® Ban on Nude Dancing on Governorâ€™s Desk
1. Introduction
72
74
Stages of NLU
P(acoustic signal | word sequence) x P(word sequence)argmax
...
The trophy would not fit in the brown suitcase because ...
ï‚§ Goal: find most likely sentence (S*) given the observed sound (O) â€¦
ï± to assign semantic roles (different from grammatical roles):
ï± I ate spaghetti with my sister.     <accompanying person>
Slide 62
Slide 71
ï® What if a sequence never appears in training corpus? P(X)=0
dinner party, Friday May
46
ï® a non-terminal designated as the starting symbol
4. Linguistic features for NLP
N: size of the corpus
I need new batteries for my mouse.I need new batteries for my mouse.
ï± next word based on past words
42
P(â€œche bella cosaâ€) with the Spanish LM
In Speech Recognition
100
ï± Ex: Silence of the lambsâ€¦
P(â€œche bella cosaâ€) with the Italian LM
want .0014 .00035 .278 .00035 .0025 .0031 .00247
= estimating P(vj|sk) and P(sk) from a sense-tagged training
Where we are today
food 19 0 17 0 0 0 0  C(food)=1506
ï® Solution: smoothing
Slide 36
May
the  cat Prep-NP
ï® but there are so many of them that too much probability mass is
ï± World knowledge
Statistical NLP
1st approximation
ï± ex. the, in, and, over, beyondâ€¦
ï® Open (lexical) class words
16
Slide 51
Information Extraction & Sentiment Analysis
.215 .00019 .0028
NP-VP       VP   Aux-NP-VP
ï® 5-grams
Summarization
B  N
Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014
Syntactic
82
ï± The meaning/sense of words is not clear-cut
8
S
101
ï® E.g.
ï± with 100,000 words, the probability of each word at any
i.e. nb of cells in the matrix
Translation from Stanfordâ€™s Phrasal:
Add-one: Example
The professor sent the student to see the principal becauseâ€¦
ï® The men next to the large oak tree near â€¦ are tall.
ï® Local HS Dropouts Cut in Half
Problem: Multiple parses
â€¦he wanted to see him.
Best roast chicken in San Francisco!Best roast chicken in San Francisco!
ï® Quantifier Scoping
ï‚§ Connectives such as â€œifâ€, â€œhoweverâ€, â€œin conclusionâ€
ï® in a training corpus, we have 10 instances of
2nd Invasion of NLP, by Deep Learning
ï± having a single non-terminal on the LHS and one or more
ï± E.g. Overlapping of word senses across languages
ï± John visited Paris. Then he flew home. He went to
ï® Knowing that Sue â€œswallowedâ€ helps narrow down
the next event to occur in the sequence
ï® e.g. first-order predicate logic, conceptual graph, embedding...
ï® Teacher Strikes Idle Kids
sentence in the language
P(acoustic signal | word sequence) x P(word sequence)
â€¦for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...
ï® Intuitively:
9. John ate the cat
affordability
aback 26 1 6 0 â€¦ 12 2
A 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
8I)|P(I
ï± NP --> D N
nice and compact to carry!
Date:   Jan-16-2012
ï± He is trying to find out.
ï± P(more | come across) = 0.1
I want to eat Chinese food lunch â€¦ Total
ï± â€œcome across 3 menâ€ --> prob = 0
Word Sense Disambiguation (WSD)
Hi Dan, weâ€™ve now scheduled the curriculum meeting.
Machine Translation
ï± P(a | come across) = 0.1
ï® Take a window of n words around the target word
87
ï® Assume V = 50
ABC has been taken over by XYZABC has been taken over by XYZ
for long sentences
mostly solved making good progress Good progress by
ï® What words are available in a
ï± S, NP, VP, PP, D, N, V, ...
â€¦playâ€¦ instrument
ï® Markov approximation is still costly
Introduction to
ï± â€œmountainâ€? â€œtreeâ€?
Example: Language Identification
Whatâ€™s a Language Model?
ï± Number of fivegrams: 1,176,470,663
â†’ Worksheet #10 (â€œParsingâ€)
(6) PP -- > Prep N
ï± VP: verb phrase       eat an apple, sing, leave Paris in the night
Current Research area: see Winograd Schema Challenge
1 14 1 1 1  3437
Slide 100
since the camera is small and light, I won't need to carry around
ï± P(world|BANK1) = 1/30 P(world|BANK2) = 0/12
to be very delicate in the handling of this camera
previously unseen events
â€œAt last, a computer that understands you like your motherâ€
ï± breadth-first: compute all paths in parallel
Slide 73
ï® Discourse Analysis
ï± (I eat) (eat I) (I sleep)
into my pyjamas, I donâ€™t know.â€
ï± Number of unigrams: 13,588,391
ï® 3
perfume.  Then he flew home.  He went to Walmart.  He
to see in a few classes
as) across C(come  across) come |P(as ï€½ï€½
syntactic constituents
ï® tries to explain what the speaker is really expressing
ï® Closed (functional) class words
ï® Bottom-up parsing /
8. John ate the N
Helping human translators
ï® Bayes decision rule:
... it was too small.
Slide 92
Create new Calendar entry
ï® The cat eats the mouse = The mouse is eaten by the cat.
Coreference resolution
Question answering (QA)
ï® Stolen Painting Found by Tree
1. Train a character-based language model for Italian:
kj
Word sense disambiguation (WSD)
size and weight
ï± P(X | come across) = 0  where X â‰  â€œasâ€, â€œmoreâ€, â€œaâ€
3. PN V the cat
not 1
Remember this slide...
CAUSE Because I was sick, I could not do my assignment.
stemming
20
ï® Speech Recognition
ï± â€œJust then, the white â€¦â€
Slide 102
ï± [Happy cats] and dogs live on the farm
Depth
marker, â€¦
Depth-first vs Breadth-first
Slide 22
17
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831
ï± possible variations: case sensitivity,
Lexicon:
ï® Output:
2. Natural Language Generation
tagging
â€¦he was throwing paper balls in class.
together?
Why is NLP hard?
34
ï® second-order Markov models
ï® Hi dear, how are ... helicopter? laptop? you? magic?
s
ï® P(â€œIâ€™d hike a toffee with 2 sugars and silkâ€) â‰ˆ 0.000000001
Slide 44
over nb of all occurrences of
log(P(British|eat)) + log(P(food|British))
Slide 80
â€¢ Rules are developed by hand in collaboration with linguists
ï® The man saw the boy with the telescope.
ï± A word may denote different things (ex. chair)
John 1
Slide 7
ï® A Language model is a n-gram model over word/character
â€¦         total = 10,000
To: Dan Jurafsky
Probability of â‰  pronunciations
ï± Number of sentences: 95,119,665,584
ï® i.e. To predict the next event in a sequence of event
Slide 88
Slide 49
kill 1
ADJ         ADJ    NOUN  VERB      ADV     ADJ         ADJ    NOUN  VERB      ADV
understands you.
ï® but some words are more frequent then othersâ€¦
ï® The man next to the large oak tree near â€¦ is tall.
character/word sequences
ï® British Left Waffles on Falkland Islands
ï± with pre-classified documents
6. NP V ART N
LSïƒ
ï± Every man loves a woman.
Slide 53
Q. How effective is ibuprofen in
48
ï± --> need to pick the most probable parse one from the set
as well as your mother
19
ï® Used when the past sequence of events is a good indicator of
ï± NP: noun phrase      the little boy from Paris, Sam Smith, I,
to any other rules for VP
Example (with add 0.5 smoothing)
92
78
some training corpus
given to unseen events
ï± Number of tokens: 1,024,908,267,229
I want to eat Chinese food lunch â€¦
1. Lexical Semantics
Slide 28
ï± using a language model > trigram is impractical
Googleâ€™s Web 1T 5-gram model
ï® Spelling correction
)...wC(w
C 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ï® But has severe limits to understand meaning of text...
ï± Features?
ï± â€¦ â€¦
bought some underwear.
Linguists
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
ï® And a total of N = 10,000 bigrams (~word instances) in the training corpus
sentences?
ï® We can use Bayes rule to rewrite this as:
ï® Juvenile Court to Try Shooting Defendant
27
The Modern Land of
E 0.0097 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ambiguous word
Deep Language Processing
ï± Very ambiguous
-- > (Maddy), (my dog), and (Samy)
Slide 79
Enter Source Text:
4. PN V ART cat
Deep Learning
Heuristic
ï± Conjunctions and appositives
49
zoophyte 0 0 0 1 â€¦ 0 0
3. Given a unknown sentence â€œche bella cosaâ€  is it in Italian or in Spanish?
ï® but does not take word order into account.  This
1-n1n ï€½
4. Highest probability  language of sentenceâ†’
ï± cat, mouse, nurses, eat, ...
â€¦violinâ€¦ instrument
ï® a representation of context-independent, literal meaning
ï± Syntactic dependencies
ï± Output = text
Hide (agent=child, patient=candy,
ï® so instead of multiplying the probs, we add the
WALLACHIA AND MOLDOVIAâ€
29
eat .00039 .00039 .0009 .00039 .0078 .0012 .0208
ï® Testing phase:
â€¢ But the linguistic features are hand-engineered and fed to the ML model
I 8 1087 0 13 0 0 0  C(I)=3437
ï± goal:
Word Freq.
ï® Training a NaÃ¯ve Bayes classifier
https://en.wikipedia.org/wiki/Winograd_Schema_Challenge
Slide 59
CS &
the rose is red /  red the rose is
ï® Grammatical roles:  subject, direct object, ...
Slide 24
Slide 74
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html
ï± (?) I am going to Concordia.  I need butter.
ï® every previously unseen n-gram is given a low probability
ï® â€œSue swallowed the large green ______ .â€
Artificial Intelligence:
2. Bag of word model
ï® I eat.  I sleep.
to 1
ï® Many possible parses for a single sentence happens
ïƒ·
across) C(come
Grammar:
Y 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
Mary did eat apples.
ï® See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
a 0 0 0 0 â€¦ 8 5
ï± Semantic dependencies
Information Extraction
parses
events (grams/units/items)
ï® But, how far back do we look?
ï® Heuristic search:
S2: Translated this!
WSD as a Classification Problem
ï® i.e., going back 3 words before helps
ï® product of probabilitiesâ€¦ numerical underflow
ï® a set of re-write rules
eat 1
Correct parse 1 Correct parse 2
ï® each word has an equal probability to follow any
Add-one: Example (conâ€™t)
given point is .00001
flash
ï® Hospitals are Sued by 7 Foot Doctors
space to unseen events
Probability of a parse tree
ï®  Input:
add-one bigram conditional probabilities:
N=10,000
ï± The child hid the candy under the bed.
ï‚§ â€¦
ï® 24 GB compressed
Slide 39
source:  Manning, and SchÃ¼tze, Foundations of Statistical Natural Language Processing, MIT Press (1999)
ï® So use a window of words around the target word as
Slide 98
before considering another
Slide 19
Sentiment analysis
ï® so we have:
t
47
5. John V NP
Youâ€™re invited to our
1  )w w (w C  )w w (wP n1 2n21Add1
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) +
â€œingâ€  is more probable than in
ï± ...
I have to go to the university.  I need butter.
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
ï± I ate spaghetti with lots of appetite. <manner>
ï± He is trying to fine out.
Menu
French
ï± a parsing strategy:
text
ï® hypothesis: texts that resemble
P(acoustic signal)
1. Speech Recognition
ï® Won Jeopardy on February 16, 2011!
G. Marx, Animal Crackers, 1930.
ï± I ate spaghetti with meat balls.  <attribute of food>
Artificial Intelligence:  Natural Language Processing
ï€½
33
ï® understanding how people use language socially
Slide 37
ï® Size:
Information
breadth first
correct sentences
problem
appearing in windows of Sk
ï± P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37
meaning/semantic
ï® Lexical Semantics :
ï® models the order of the events
apples.
ï® Assume we translate from fr[foreign] to English  i.e.: (en|fr)
Retrieval
facts, â€¦) modifies our understanding of
leg
Some Difficulties
27 at 8:30
ï® n-grams take [a bit of] word order into account
cewordsequen
How knowledge about the world (history,
Natural Language Processing
ï® Goal:
ï± Information Retrieval (eg. google search)
ï± the probability of expanding VP by a V NP, as opposed
This is only a matter of time.
en
2. Count words and build model
unsmoothed bigram counts:
English Parts-of-Speech
individual words?
Machine Learning
ï® We painted the wall with cracks.
36
The man saw the boy with the telescope.
â€¦         ...
ï® Sentences can be very ambiguousâ€¦
ï® Iraqi Head Seeks Arms
ï± Clean and tokenize
P(wn|wn-1wn-2)
ï® ie: events = words  or  events = character
Problem with n-grams
63
ï± NP --> PN
2. Compositional Semantics
over the total nb of features
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
38
ï® S is the set of possible senses for the target word
ï® How to arrange words
MOST FAMOUS NOVEL
ï± 1 time, followed by â€œaâ€
not really part of the scene, just as a sort of nod to gringo
ï® The idea is to give a little bit of the probability
ï± a grammar:
â€¦stripedâ€¦ fish
Slide 8
WILLIAM WILKINSONâ€™S
language) share similar
ï® Artificial
Buy V1AGRA â€¦Buy V1AGRA â€¦
ï± some languages do not have all these categories
Slide 58
A Parse Tree
Find: The most likely English
ï® ex: bass = fish, musical instrument, ...
ï® set of non-terminal symbols
32
Slide 66
ï® 24
95
ï± V = {a, aardvark, aardwolf, aback, â€¦ , I, â€¦, want,â€¦ to, â€¦, eat, Chinese, â€¦, food, â€¦, lunch, â€¦,
14
1088
Learning
Factors of Training Corpus
ï® breadth-first:
source: Robert Dale.
ï± P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12
Slide 65
44
ï± so that there is a little bit of probability mass left over for
i.e. nb of n-gram tokens in training corpus
S3: â€¦
Slide 25
Slide 27
like 1
ï± P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)
ï® (set of) parse trees
Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829
End:    11:30am
51
(mid 1980 â€“ circa 2010)
Find: The most likely word/sentence â€“ S*
ï± How do we deal with sentence boundaries?
ï® 12
99
n1
ï® â€œthe large green ______ .â€
Slide 18
ï± Large and open vocabulary (new words everyday)
83
â€¦salmonâ€¦ fish
ï± generally function/grammatical words
Slide 78
ï®  Discourse tagging can be viewed as typical classification problem
ï± 8 times, followed by â€œasâ€
ï‚§ ie. pick the sentence with the highest probability:
in a specific sentence
Compositional Semantics
Attributes:
D 0.0042 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ï® ie. The meaning of individual words
= finding the most likely sense k
Slide 38
ï± NP --> Pro
ï® Goal: choose the most probable sense s* for a word given a vector
Walmart. He bought Mary some expensive perfume. He
ï± characters = 26 letters (case insensitive)
trees
ï® sk is a sense in S
ï® Syntax is not enough
96
ï± Complex syntactic structures
Slide 76
ï± Output = acoustic signal
79
2. Speech Synthesis
animal
ï± PP: prepositional phrase  in the morning, about my ticket
â€œcome acrossâ€
Acoustic model --
Acoustic model
= 2,621,456
ï® An n-gram model is a probability distribution over sequences of
Slide 94
ï® first-order Markov models
Slide 84
Deep Neural Networks applied to NLP problems
8. NP VP
ïƒ¥
ï± P(and|BANK1) = 1/30 P(and|BANK2) = 0/12
2. Determine how words are put together to form
Slide 14
ï® Kids Make Nutritious Snacks
ï± new_count(n-gram) = old_count(n-gram) + 1
add-one smoothed bigram counts:
â€¦
S1: How to recognize speech.  ?
Some Adjustments
febrile illness?
ï® used in many applications:
âœ—
Translation model
P(en) x en)|P(fr argmaxen*
45
ï® probability of an n-gram involving unseen words will be zero!
53
A B C D â€¦ Y Z
ï± In English character sequence
ï± Number of fourgrams: 1,313,818,354
â€¦Welcome to the Bank/BANK1 of America the nation's leading financial institutionâ€¦
52
Breath
ï± Most likely sense of the word
lunch 4 0 0 0 0 1 0  C(lunch)=459
ï± AdvP: adverb phrase slowly, really slowly
Slide 96
Model Applications
ï± nouns, main verbs, adjectives, adverbs
â†’ Worksheet #10 (â€œSentence Probabilityâ€)
speech
Language model
ï® most widely used value for ğ›¿ = 0.5
ï® Map sentences to some representation of its
91
Slide 101
40
)count(s
to .00082 .0002 .00226 .1767 .00082 .0002 .00267
corpus
mother.
Slide 61
The Ancient Land of NLP
ï± use machine learning techniques (ex. NaÃ¯ve Bayes
Slide 60
Slide 97
ï± new words can be added easily
Smurf talk on youtube:
https://www.youtube.com/watch?v=7BPx-vl8G00
7. NP V NP
Housing prices roseHousing prices rose
437 3
ï± NP: âˆ€i âˆ‘i P(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
ï± Number of trigrams: 977,069,902
Slide 85
Event:  Curriculum mtg
â€¦Welcome to America's Job Bank/BANK1 Visit our site andâ€¦
ï‚´ï€½
Forest
ï± the more, the better
very oftenâ€¦
ï® Not in NL:
In Statistical Machine Translation
ï® i.e., Meaning of individual words
ï® I shot an elephant in my pyjamas.
ï± Even simple sentences are highly ambiguous
translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310
to 4 1 11 861 4 1 13  C(to) + |V| = 4872
ï® generated from 1 trillion words
Search
zoom
Using World Knowledge
= automatic processing of speech
a sentence/discourse
ï® Pretend we have seen every n-gram at least once
(10) Det --> the
ï® Training corpus (context window = Â±3 words):
relations between
Slide 35
Slide 83
ï® Pragmatics
1. Data preparation:
ï® Language Identification
Slide 3
â€¢ Rules are developed automatically (using machine learning)
ï‚§ Tense of verb (future, past)
âœ“
Einstein met with UN officials in PrincetonEinstein met with UN officials in Princeton
13
ï® Assume a vocabulary of 1616 (different) words
65
meaning of phrases and
4. John VP
Slide 11
1-n1
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
XYZ acquired ABC yesterdayXYZ acquired ABC yesterday
John did not kill Mary.
location=under_the_bed, time=past)
maintenance manuals
93
2. The computer understands that
Machine
http://en.wikipedia.org/wiki/Google_Ngram_Viewer
purpose, â€¦)
parsing
ï± e.g., logics, semantic network, embeddingâ€¦
https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-
punctuation, beginning/end of sentence
Rule-based NLP
ï± bottom-up (data-directed): words --> grammar
Turing was a great mathematician living in
Slide 4
Neural networks
S4â€¦
88
)s,count(v
ï® Because it is ambiguous:
Examples of Discourse Relations
Part-of-
each other (same author, same
Speech
Date: January 15, 2012
ï± s* = argmaxsk P(sk|V)
Add-one Smoothing
Ã©tape
Island
Example: John ate the cat
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014
sentence â€“ en*
text?
(Dracula)
Castro Theatre at 7:30.
unseen examples one of a fixed number of senses
)count(word
Slide 5
chair
(11) N --> cat
)w w (w C  )w w (wP n1 2n21AddD
aardvark 0 0 0 0 â€¦ 0 0
22
ï± Not tolerant to errors (ex. Syntax error)
â€¦
Great Britain. He was an atheist as well as gay.
ï®  Main Constituents:
77
surrounding sentences?
ï®  These phenomena can quickly increase the number of
ï® The man next to the large oak tree near â€¦ talks rapidly.
PP attachment:
Slide 46
INSPIRED THIS AUTHORâ€™S
ï® â€¦
O)|P(SargmaxS*
Applications
(1) S --> NP VP
ï® there may be long-distance dependencies.
Bill Gates passed away last night.
ï± S: sentence       The boy is happy.
jambe pied
ï® set of terminal symbols
Slide 93
is the bag of word approach.
number of discourse relations (categories)
World Knowledge
Top down ïƒ¼ ïƒ¼ ïƒ¼
How to relate the meaning of sentences to
Slide 86
It will be in Gates 159 tomorrow from 10:00-11:30.
ï® Speech Processing
ï± but after a while, not much improvementâ€¦
â€¦lounging against verdant banks/BANK2 carving out the...
ïƒ¸
Slide 54
ï± next move of player based on his/her past moves
55
11
ï± Smaller vocabulary
Given: Foreign sentence - fr
Do you have a quarter?
argmax:
https://en.wikipedia.org/wiki/Claude_Shannon
ï® N-by-N-by-N matrix of probabilities/frequencies
What is the
log of the probs
ï± P(BANK1) = 5/7 P(BANK2) = 2/7
ï± |V| = 1616 words
66
1. S
ï± (<s> I) (I eat) (eat <s>) (<s> I) (I sleep) (sleep <s>)
ï± compute 1 level at a time
zoophyte, zucchini}
)s|P(v log  )P(s logargmaxs*
Slide 56
model
And Even More Examples of Ambiguity
C(I) + |V| = 5053

...
è¿™ ä¸è¿‡ æ˜¯ ä¸€ ä¸ª æ—¶é—´ çš„ é—®é¢˜ .
is good
Slide 87
a CFG consists of
68
SF?
journey
https://www.youtube.com/watch?v=sbJ89LFheTs
Summary of Parsing Strategies
zucchini 0 0 0 3 â€¦ 0 0
Automatic Language Identificationâ€¦
ï± bigram needs to store 400 million parameters
56
Slide 32
ï® Genre (adaptation):
ïƒ¥ï€«ï€½
50
ï® Predict the next word/character given the n-1 previous
B 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
ï® assign syntactic structures to a sentence
ease of use
BANK1 BANK2
â€¢ Applications: Information Retrieval, Predictive Text / Word Completion,
ï± Heuristic search
lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075
â€¦The Asian Development Bank/BANK1 ADB a multilateral development financeâ€¦
ï± Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) â€¦
(categories)
â€¦Today the World Bank/BANK1 and partners are calling for greater reliefâ€¦
ï® Parallel problems to syntactic ambiguity
possible parse trees!
of  Deep
ï± AdjP: adjective phrase    really funny, rather clear
tokenisation Decision trees
.00019 .00019 .00019
guess how thatâ€™s done?
Subject: curriculum meeting
3. The computer understands you
3. PN VP
39
(12) VB --> eats
classifier, decision tree) to train a system
6. John ate NP
ï± words & punctuation
Letâ€™s go to Agra!Letâ€™s go to Agra!
Nb of  occurrences of sense k
PERSON              ORG                      LOCPERSON              ORG                      LOC
Do you want a ticket?
previous words (the history)
City of
ï® Since denominator is the same for each candidate S, we can ignore it for the
ï® See discussion:
Parsing Strategies
ïƒ¨
Problem: Data Sparseness
language?  gfiioudd  / table
Slide 82
Building n-gram Models
words/characters.
ï® depth-first: exhaust 1 path
Information extraction (IE)
ï® So for:
ï± E.g.: figures of speech, â€¦
Slide 47
Slide 99
â€œAN ACCOUNT OF THE PRINCIPALITIES OF
Processing
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)
ï± 1 time, followed by â€œmoreâ€
â€¦
59
ï® bigrams (characters) after 100â€™s million words
ï± aka stop words
I can see Alcatraz from the window!I can see Alcatraz from the window!
ïƒ¦
ï± P(off|BANK1) = 0/30 P(off|BANK2) = 1/12
ï¤
P(wn |w1w2â€¦wn-1)
Syntactic Parsing
N+|V|2 = 10,000 + (1616)2
meaning
62
000 10
ï± General Kaneâ€¦ person     but  General Motors â€¦ corporation
ï± Simple syntactic structures
ï® In logics:
ï± Non-ambiguous
Slide 26
ï® Natural language is not linear ....
Part-of-speech (POS) tagging
4
Bigrams
unsmoothed bigram counts (frequencies):
Slide 77
terminal or non-terminal in the RHS
... it was too big.
Slide 33
Another Classification Problem,  again!
Limits of BOW Model
want 3 0 786 0 6 8 6  C(want)=1215
First
ï® WSD can be viewed as typical classification
d
Dialog  Where is Citizen Kane playing in
PURPOSE To use the computer, get an access code.
ï® N = size of the vocabulary we are using
â€¦playerâ€¦ instrument
ï® Most likely relation in the sentence (none, condition, contrast,
Slide 9
ï± P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37
Slide 21
Slide 68
Linguistic features used for what?
Semantic interpretation:
10
Named entity recognition (NER)
I .0018
expectations perhaps.
2nd approximation: unigrams
those heavy, bulky professional cameras either!
73
(3) S --> Aux NP VP
ï® Maybe we should take word order into account...
Slide 70
LS
Who is Bram
ï® Maddy, my dog, and Samy
ï® Determining which sense of a word is used
3. n-gram models
Features for WSD
kjk
ï® A simple model where word order is ignored
The Land of Statistical NLP
Example of a PCFG
3rd word
to 3 0 10 860 3 0 12  C(to)=3256
Semantic Interpretation
possibilities
ï® The/DET rose/NOUN is/VERB red/ADJ.
ï® Top-down parsing /
CONDITION  If it rains, I will go out.
â€¦riverâ€¦ fish
â€¢ And the linguistic features are found automatically!
2. PN ate the cat
Discourse Analysis
another
recognition
P(O)
â€¢ Cognitive approach
2nd word
ï® Natural Language Processing
ï® Intuitively, P(VP â†’ V NP) is:
ï® Training:
Parsing
64
ï® World Knowledge
ï® take into account the frequency of the word in
ï± relatively fixed membership
Given: Observed sound - O
43
SEQUENCE Do this, then do that.
67
ï± trigram needs to store 8 trillion parameters
6
ï® goes beyond the literal meaning of a sentence
â†’ Worksheet #10 (â€œWord Sense Disambiguationâ€)
did 2
ï± decrease the probability of previously seen events
Slide 52
Village of
Machine translation (MT)
be explicitly stated in order to make a text coherent.
Symbolic methods / Linguistic approach / Knowledge-rich approach
ï± that learns a classifier (a function f) to assign to
