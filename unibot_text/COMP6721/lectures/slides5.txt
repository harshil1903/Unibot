
 Not all errors are equal
2. Abduction

Slide 75
X1 1 0 0 ?
X4: Jeff No Yes No Yes No
Abduction
Intuitively…
Slide 72
 Split the examples so that those with different values
1,
71
Slide 2
 From A  B and A, we conclude that B
 Shape and color are the most discriminating
2. Divide collection into two disjoint sets:  training (90%) and test (10% = 1/k)
 Noisy Input:
Example
computational units
Noisy Input
Types of Machine Learning
Recall=
http://www.rulequest.com/Personal/
 Let’s assume we pick Color for the root…
80
K-means
 User selects how many clusters they want… (the value of k)
Output f(X)
 4 tests instead of 9
57

Slide 40
 But we get a reward when our learned f(X) is right, and we try to maximize the reward
 k number of clusters
Inductive Learning
 If you already have a good idea about the answer (e.g. 90/10 split)
 Raining: raining outside
 Information Retrieval (eg. Image search)
Why -p(x)·log2(p(x))
99 - )p(x)logp(xH(X) 22
 Computer vision (eg. Object recognition)

 % of instances of the test set the algorithm correctly
Model says…
baWHM
81
Real ML applications typically require hundreds, thousands or millions of examples

 average uncertainty of a RV
Decision Boundaries of
 Partition them into regions of high density
85
class F4?
0- x
c2 c3
35
 Training data:
attribute
Deduction
❌ ❌ ❌ ✔
86
11,0Hblue)  Color |H(S
 
example of the general case
attribute A (for each value v from Values(A) )
clustered arounds certain intersections
https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b
length
Entropy (or information content)
classifies
Slide 67
re-compute new centroïds
 We are given a training set of (X, f(X)) pairs
Slide 55
84
2,
training data but irrelevant to the problem.
True Negative
76
18
n Color
 Clustering algorithm
been assigned)
i2i )p(x)logp(xH(X)
2H(S) 22 
clustering algorithm performs better in
judgments in new situations
 Recall, Precision & F-measure
15
98
 We construct a general explanation based on a specific
possible value of the selected
 Constructs algorithms that learn from data
 patient complains about some symptoms… doctor concludes a disease
 This type of assumption is called inductive bias
1log
instance is in class C True Positive
9
A Small Example (3)
23
...gain(alt) ...gain(bar) ...gain(fri) ...gain(hun)
 if R = 100%  P = 10% M = 55% (not fair)
 Alternate: another suitable restaurant nearby
X
Induction
feature space
 If value is No… all output= No
the data acquisition)
X3 8 0 1 ?
 Intelligence arise from having a large number of simple
 “It is better to risk saving a guilty man than to condemn an
1)PR(βF 2
clusters.
Slide 71
 when β > 1, precision is favored
where there were polluted wells – thus
 If value is Italian… all output= No

between them, and “dissimilar” to data items in other
60
Which Tree is Best?
 Top-down construction of the decision tree
Accuracy=
Slide 43
Slide 17
4log
25
21gain(type) 
41
Slide 89
Slide 48
Socrates is mortal.
outputs
meaningless regularities in the data that are particular to the
 measured in bits
 i.e. the smallest tree on average
31
fair coin -> high entropy
examination of only a few member of the class.
 We are given a training set of (X, f(X)) pairs
 than you average the results
F1?
Today
 aka Natural Deduction
Some Words on Training
 Size of training set
7
 In all types of learning… watch out for:
 Ex:
initial 3 centroïds (ex. at random)
 Model is not expressive
 Once the feature is selected for
of all possible decision trees
 how?
small nose big teeth small eyes moustache f(X) = ?
system in a classification (or regression) task

 let β2 = a/b
Slide 12
big nose big teeth big eyes no moustache f(X) = not person

 There is no way to fit a linear
3Shape)|H(S
 The locations indicated that cases were
75
A Small Example (5)
C6 0 0 5 0 10 85  100
tree
 Patron:
(FN)
 Let X be a discrete random variable (RV) with i possible
 Testing error is high

Slide 10
 
 How?  … many algorithms (ex. k-means)
Slide 64
accomplish
K-means: Summary
Slide 20
Slide 13
Drinks? ‘A’ this year?
✔ ❌ ✔ ✔
trees
54
“It is vain to do more than can be done
 do recursively for subtrees
class F6?
4. Measure performance with the test set
bass
 ex: 10-fold cross validation
37
class F5?
 WaitEstimate: estimated wait by host (0-10 mins, 10-30, 30-60, >60)
Euclidean distance
rigged coin -> low entropy
Types of Learning
89
all examples associated with a
Slide 34
Slide 50
Slide 42
Slide 15
1
the most popular algorithms, due to its
Xx
Slide 31
 Cluster/group the features of the test data into a
 Medical Diagnostics
 take weighted harmonic mean
5. Repeat steps 2-4, with the 10 different portions
94
 Inference: process of deriving new facts
 Conclusion about all members of a class from the
Slide 1
and improve their performance the more tasks they
 Most work in ML
in class C Is not in class C
0.31150.6885-1  Shape)|H(S - H(S)  )gain(Shape
Slide 23
or)values(Col  v
Slide 95
1βWHM 2
 Data mining
salmon
Euclidean Distance
 cannot take mean of P&R
Techniques in ML
S
(A) Two natural clusters                                      (B) k-means clusters
Slide 16
Feature 2
2. Decision Trees
http://www.cognub.com/index.php/cognitive-platform/
edges
Slide 6
John is drunk.
Rb
(FP)
H(S) Color) gain(S, 
4. Repeat Steps 2 and 3 until none of the data
Slide 29
ii qpd
 Some values of features are incorrect or missing (ex. errors in
 Ex: X = [nose:big, teeth:big, eyes:big, moustache:no]
op
R)P(β
 Detecting credit card fraud
 From A ⇒ B and B, we conclude A
0H x
 run k experiments, each time you test on 1/k of the data, and train on the rest
average of the cluster
1 2 3 4 5 6 7 8 9 10
X6: Simon No Yes Yes Yes No
John does not walk straight.
 Learning = crucial characteristic of an intelligent
 Notion of entropy (information content)
C5 0 0 3 2 92 3  100
x?
Slide 69
y
70
 Ex:
 Can be seen as learning a function
bits 0.08
26
class F7?
1. Introduction to ML (contd.)
feature to split a given set of examples
 So that given a new X, you can predict its f(X) value
Clustering Techniques
90
Essential Information Theory
 Log 0 = -∞
 Each vector can be visually represented in an n-dimensional
 Idea: not all paths are equally important/likely
i
n
 Noisy input
node in the tree
 Partition the examples using the

→ Worksheet #4 (k-Means Clustering)
 hungry:
 n number of data points
Slide 30
Works
Slide 57
training examples are well
decision regions
Other possible decision
values(A)  v
Slide 81
 In Reinforcement learning
possible values of this attribute,
Slide 90
entropy of a fair coin toss (the
 We can also underfit data, i.e.
3. When all data points have been assigned,
 Choose the attribute that has the largest
 With small k and t, linear performance on practical
74
Student ‘A’ last
From A C∧   ⇒ B and A D ∧  ⇒ B, we conclude A⇒B
Ockham’s Razor Principle
n Circle: 2+ 1-    n Square: 0+ 1-
test instance
69
No one knows the correct clusters!
Slide 62
 If value is French… all output= No
46
Slide 25
100
42
HM is high only when both P&R are high
(root of the tree)
instances change group.
→ high entropy
 Algorithm is sensitive to outliers
A Learning Curve
 ex: Naïve Bayes Classifier
Entropy
wr: weight of R wp: weight of P a = 1/wr b= 1/wp
each with a probability of 1/2
6. Average the results of the 10 experiments
 for only 2 outcomes x1 and x2, then 1 ≥ H(X) ≥ 0
 For all k clusters, chose the
result, place these features (as questions) in nodes of
Slide 36
Features (X) Output f(X)
 If you have no idea about the answer (e.g. 50/50 split)
❌ ❌ ❌ ❌
 Developed by Shannon in the 1940s
t1 Feature 2
[Ockham, 1324]
& k-means Clustering
16
 Neural networks
 search the space of all decision
particular training data at
Slide 51
 Reservation: reservation made
What’s the size of a tree?
 the average length of the message needed to transmit an
 Fast, simple, and traceable
 Goal: learn whether one should wait for a table
class class class class class class class class
82
8
expected size of the subtrees rooted at its
These points represent initial group centroïds.
 a possible metric is the
28
 = learning from examples
Error Analysis
 Speech Recognition / Synthesis
Socrates is a man.
empty tree
 The organization of unlabeled data into similarity groups
n Size
 if R = 50%    P = 50% M = 50%
0.6885  (0)
  In Unsupervised learning
n Shape
-
 Hungry: whether one is hungry
the tree
ID3 uses Maximum Information-Gain:
97
?
87
children nodes, one for each
simplicity and efficiency
C1 C2 C3 C4 C5 C6 … Total
Remember this slide?
centroïd…
Values(Color) = {red,blue}
… …
 Start at leaf, go up to the root and count the number of
t1
A First Decision Tree
→ Worksheet #4 (Information Content)
 Supervised learning
 Type:
Smaller trees are better
 Each attribute has a fixed, finite number of possible values
 Use a confusion matrix / contingency table
Slide 73
Example: The Coin Flip
 can be seen as searching the space
Next Feature…
X1: Richard Yes Yes No Yes No
H(S) 22 
 Most used strategy: information theory
r
Slide 92
 Conclusion is one hypothetical (most probable)
 Use only discriminating features as questions in a big if-then-else
(information gain)
0
→ Worksheet #4 (“Information Gain”)
F3?
 So patron  may lead to shorter tree…
 t number of iterations
0 1 2 3 4 5
All CS students are smart.
the desired output
 Patrons: how many people are present (none, some, full)
 Learning heuristics for game playing
 Types of logical inference:
20
 Fri/Sat: true on Fridays and Saturdays
Big Red Circle +
Confusion Matrix
2222 
Slide 22
tree should be attribute Patrons
17
 Not sound… but may be most likely explanation for B
smallest information gain)
average of data points in the cluster
)p(x)logp(xH(X) i
 We are only given the Xs - not the corresponding f(X)
4. Unsupervised Learning: k-means Clustering
 
0.3115  )gain(Color
A Small Example (1)
 when all classes are equally important and represented
a
 Strengths:
34
Slide 44
space
2
  Russell & Norvig: Sections 18.3, 18.4
Small Red Circle +
Slide 80
5
 Overfitting/underfitting the training data
Slide 7
2log
C3 0 1 94 2 1 2  100
 i.e., the attribute that will result in the smallest
bit 1
 Compute the centroïd of each region as the
Feature 1 > t2
0log
i2i 
 Note: choosing one function over another beyond just looking at the
 A cluster is a collection of data items which are “similar”
 distance between 2 pts
Slide 88
"discriminating power"
year?
information gain
2H x
12
source: Norvig (2003)
attributes (i.e. highest information gain)
 A weighted combination of precision and recall
Slide 53
 measures the amount of information in a RV
Inductive Learning Framework
Info on last year’s students to determine if a student will get an ‘A’ this year
48
actually labeling them
19
 Very well-know algorithm is ID3 (Quinlan, 1987) and its
92
78
A Small Example (4)
 The "discriminating power" of an attribute A given a data set S
 Comparing different clustering algorithms
 X = <color, length>
 Testing error is high
a1 a2 a3 Output
Feature 2 > t3
Slide 28
 Natural Language Processing (eg. Spam filtering)
 i.e., perform tasks that were not explicitly programmed
Weakness of k-means
points with very different characteristics
 they are too tuned to the
Why use k-means?

exp1: train test
1. Deduction
27
Slide 85
??
 in reality… disease ⇒ symptoms
 Learn without labeled examples
Example (in 2-D… i.e. 2 features)
Shape?
Slide 79
training set is called inductive bias (eg. prefer "smoother" functions)
1H  )p(x)logp(xtoss) coin H(fair
outcome xi of that variable
 But you can:
Slide 4
 Probabilistic Methods
1
bits 0
29
Choosing the Next Attribute
 Extreme case: “rote learning”
S2
Slide 74
 But, can be seen as  hypothesis construction or generalisation
 No teacher involved / Goal: find regularities among the Xs (clustering)
agent
Finding the Best Tree
 Also called parallel distributed processing or connectionist systems
and assign these subsets of the
Example 2: The Restaurant
 Efficient: Time complexity O(t·k·n)

99log

 Attribute pat (Patron) has the highest gain, so root of the

 type:
 inductive bias = making a choice beyond what the training
1H x
...gain(est)
 f(X) = function to tell if X represents a human face or not
F2?
 Where did the learner go wrong ?
 Some relevant attributes are not taken into account in the data
 If a large number of irrelevant features are there, we may find
 Inductive bias: prefer shorter
Outliers
 when β = 1, precision & recall have same importance
 If value is Italian… we need more tests
C2 0 93 3 4 0 0  100
Slide 39
(TP)
 where n is the number of attributes
Slide 98
Slide 19
 Number of leaves
 Bar: comfortable bar for waiting
47
 Use the training data to computed a weighted sum
source: Alison Cawsey: The Essence of AI (1997).
0  gain(Size)
 Let Values(A)  = the set of values that attribute A can take
→ Worksheet #4 (F-Measure)
 External Path Length
sea bass
node
4
 Input data are represented by a vector of features, X
2Hred)  Color |H(S
X3
En

Note: attribute == feature
33
general
(depends on the application, e.g., spam filtering)
climbing search where heuristic is
Slide 37
Logical Inference
split the data based on its
 Repeat for each child node until
2i
The Restaurant Example
 Extrapolate from the training set to make accurate
Special data structures
classes assigned by the learner
 Rigged coin:
 If value is None… all outputs=No
Choosing the Best Feature (con't)
and recall
X5 1 7 1 ?
 Two examples have the same feature-value pairs, but different
β
 i.e., data points that are far away from others
0x
C4 0 1 3 94 2 0  100
4,
correct class
separated
 The key problem is choosing which feature to
sea bass salmon
tr
36
c1
Evaluation: A Single Value Measure
Applications of Decision Trees
 You must find an estimate of the function f(X)  where f(X) is
Decision Trees
38
3Color)|H(S

boundary
Slide 8
What is Clustering
 Training error is low
 Can out-perform human experts in many problems
c3

 So first separate according to either color or shape
32
What is Machine Learning?
Historic Application of Clustering
split a given set of examples
95
big nose big teeth big eyes no moustache not given
 Recursive selection of the “best feature” to use at the current
A)|H(SH(S)A) gain(S,
14
X2
 Decision Trees
C1 94 3 0 0 3 0  100
 Pattern Recognition (eg. Handwriting recognition)
Slide 65
 Find a function that fits the training set well
44
Slide 27
X2: Alan Yes Yes Yes No Yes
 Given a new instance X you have never seen
Artificial Intelligence:
case.
 If value is Yes… we need more tests
51
outcomes xi
use too simple decision
0.541bits...
 

Slide 18
83
Slide 78
boundary
(A) Undesirable clusters
if R = 100% and P = 10%   HM = 18.2%
 Accuracy
decision boundary so that the
Evaluation: the F-measure
 We are not given the (X, f(X)) pairs
co

 We conclude from the general case to a specific
(TN)
0.3115  )gain(Shape
Slide 38
children
96
 Goal: maximize the nb of right answers
Slide 76
...)gain(price ...gain(rain) ...gain(res)
PR 1)(β
79
successor C4.5
if R = 50% and P = 50%    HM = 50%
problems
Slide 94
 Each example can be interpreted as a point in a n-dimensional
multiplied beyond necessity.“
Slide 84
X4 6 1 0 ?
TP+TN
Slide 14
 Discovering Genetic Causes of Diseases
X2 1 6 0 ?
small nose small teeth small eyes no moustache not given
 Longest path in the tree from the root to a leaf
 Weighted External Path Length
…
 K-fold cross-validation
(that should have
2)H(S 222
lo
53

 For only data where patron = Full

52
outbreak in the 1850s.
A Small Example (2)
t1t3
X5
one where x has the
Slide 96
 Look for features that are very good indicators of the
 User needs to specify k
 Represent each instance as a vector <a1, a2, a3,…, an>
information gain
40
predictions about future examples
2HM
overfit the data
Drunk people do not walk straight.
 Each vector X is a list of (attribute, value) pairs.
Slide 60
Slide 97
 information theory
(B=1)
Back to the Restaurant
partition data points to closest centroïd
(B) Ideal clusters
 If value is French… we need more tests
❌ ❌ ❌
 the more, the better
→ Worksheet #4 (Entropy)
1 - )p(x)logp(xH(X) 22
 They do not generalize well to
Cross-validation
 Given  pairs (X,f(X)) (the training set – the data points)
 β represents the relative importance of precision
 Simplest, but most successful form of learning algorithm
 If value is Burger… we need more tests
Clustering
Sensitivity to initial seeds
Slide 35
 Attributes
 take harmonic mean
log
 when β < 1, recall is favored
Slide 83
hair?
ML: Decision Trees
Some Intuition
P(head)
 One of the most widely used learning methods in
TP+TN+FP+FN
the new data
Size? or
13
 The number of attributes is fixed (positive, finite)
2H x
65
number of groups
1. Place k points into the space (ex. at random).
Slide 11
recalculate the positions of the K centroïds as the
q = (q1, q2, ....,qn)
 In other words… always favor the simplest answer
Pb
 where to assign a data point
 Complicated boundaries
 No clear evidence that any other
… which is called the F-measure
 Training error is high
All CS students in COMP 6721 are smart.
93
 generalize from given experiences and are able to make
 i.e. X is given, but not f(X)
bluered
that correctly fits the training data

hard?
88
21gain(pat)
→ low entropy
→ Worksheet #4 (Decision Tree)
 But is it the best decision tree we can build?
trees on average
3. Apply learning algorithm to training set
 In Supervised learning
The key problem is choosing which
 always pick the next attribute to
 in effect, steepest ascent hill-
 v
Slide 5
Intro to AI
Metrics, revisited
22
77
TP+FN
Slide 46
 …
complete tree
hand
Applications
t2
 Not sound
Precision =
Big Red Circle -
X5: Gail Yes No Yes Yes Yes
Slide 93
 X = features of a face (ex. small nose, big teeth, …)
0.918
 Type I error (FP) might be worse than Type II error (FN)
 Height of the tree
instance is NOT in class C False Negative
F4? F5? F6? F7?
Slide 86
 but after a while, not much improvement…
All CS students on vacation are smart.
F2? F3?

enough (not enough features)
Slide 54
exp3: train test train
Color
55
set
11
 Let Sv = the set of examples in the data set which have value v for
 Price: price range ($, $$, $$$)
R
3. Induction
Slide 56

outlier
 Too many to list here!

c2
Slide 87
All men are mortal.
68
56
X4
Slide 32
50
class
examples to the appropriate child
 Size is the least discriminating attribute (i.e.
class class
source: Mitchell (1997)
 John Snow, a London physician plotted the
n Big: 1+ 1- n Small: 1+ 1-
b)(a
p = (p1, p2, ....,pn)
 11 branches instead of 21
 Measure how “predictable” a RV is...
X3: Alison No No Yes No No
 So hungry is more discriminating (only 1 new branch)…
39
 Examples are given (positive and/or negative) to train a
✔ ❌ ❌ ✔
 Repeat the same process with another feature

     P1bR
Slide 82
 Recommender systems (eg. Netflix)
or)Values(Col of v each for
(or entropy reduction)
Guess Who?
False Positive
30
 Conclusion follows necessary from the premises.
Small Red Square -
1(0.918)
Slide 47
 Could be errors in the data or special data
 Without a f(X), you can't really identify/label a
 Discriminate between these groups without
k-means Clustering
In reality, the instance is…
1. Collect a large set of examples (all with correct classifications)
from a set of premises
with less… Entities  should not be
Overfitting

b
2. Assign each data point xnto the nearest centroïd.
0.6885(0)
 when one class is more important and the others
ID3 / C4.5 Algorithm
62
exp2: train test train
 Type: kind of restaurant (French, Italian, Thai, Burger)
exposing both the problem and the solution.
Slide 26
P
Slide 41
4
small nose small teeth small eyes no moustache f(X) = person
Slide 77
Unsupervised Learning
explanation for the premises
source: Tom Mitchell, Machine Learning (1997)
v SH x
FROM: Nina Mishra HP Labs
 Despite weaknesses, k-means is still one of
Slide 33
RPβ
X1
the current node, generate
node are classified
 If value is Full… we need more tests
n Red: 2+ 1-  n Blue: 0+ 1-
is a difficult task.
Slide 21
 Oh… I’m out of space
Slide 9
 To find the nearest
Slide 68
instances contain
Big Blue Circle -
practice
10
Slide 70
TP+FP
 Do this for every leaf and add up the numbers
Feature 1
 Easy to understand and implement
 Entropy (or information content)
H(S)
Black
 ML
  Unsupervised learning
 Used in medicine…
 Fair coin:
innocent one.” (Voltaire)
Underfitting
RV) with 2 possible outcomes,
 Represent test instances on a n dimensional space
1i
TP
 If value is Thai… we need more tests
Size Color Shape Output
Feature 2 > t1
called clusters.
re-assign data points to new closest centroïds
 0log0 is 0
Possible decision
smallest distance
for the chosen feature are in a different set
 Simple
Size
21
64
3
2log
43
 If value is Some… all outputs=Yes
67
6
Note: by definition,
0.31150.6885-1  Color)|H(S - H(S)  )gain(Color
Slide 52
location of cholera on a map during an
A Better Decision Tree
3. Evaluation (contd.)
