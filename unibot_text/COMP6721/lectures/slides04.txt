
:Rule Bayes with
Day7 Overcast Cool Normal Strong Yes
Young ~ Young Young ~ Young
 P(COOKING) = P(SPORTS) =
Toothache ~Toothache
Example 2
doc100000:  … stove…heat… ball…
Slide 72
 intersection A1 ∩ ...∩ An is an event that takes place if all the events
yes yes yes?  or no?
71
Slide 2
no)yyes| CavitP(Youngno)yyes| CaviteP(Toothachno)P(Cavity
Motivation
 i.e. Your belief about A given that you have no
 Testing: “the referee hit the blue bird”
2. TEST:
Example
 Given
Types of Machine Learning
 ex: the word ambulance is not conditionally independent of the
)c,count(w
Recall=
in321i
Target system 1 system 2 system 3

57
ij
1j
hy
completely ruled out !
 ex: add-1 smoothing:
A+C
Slide 40
In 1959, Arthur Samuel first proposed
 So
 union A1∪...∪ An is an event that takes place if at least one of the
tomorrow
 % of instances of the test set the algorithm correctly
61
Model says…
 c1: SPAM
yes)  Youngyes cheno| ToothaP(Cavity

)H|P(X x )P(H  argmaxH
 Score(COOKING)= log() + log(P(the|COOKING)) + log(P(referee|COOKING)) +
)H|a(P ij
Understand patterns and
activities.
using binary values for the presence/absence of words…
yes) P(Youngyes) eP(Toothach
Conditional Independent Events
Logistic Regression, KNN,
 P(stove|COOKING) =    P(stove|SPORTS) =
• for DTs: pruning level
https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b
 data set contains handwritten
 Precision: What proportion of instances labeled with the class
 Example: predict if a customer is likely to purchase
classifies
Slide 67
3. Test set ~20%
)c|P(w

X4  X4 
Slide 55
)P(c ii 
Day12 Overcast Mild High Strong Yes
 predict the weather that maximizes the probability
yes)Youngyes ) x P(eP(Toothach
 Observation: average sunset (E2)
Postal Code Recognition
 select Hi such that P(Hi | E2) is the greatest
 gives confidence in its class predictions (i.e., the scores)
18
Machine Learning History
been assigned)
certain goods according to history of shopping
ll_eventscount_of_a
 Recall, Precision & F-measure
 i.e.  A is independent of B if P(A) = P(A|B)
 % of instances of the test set the algorithm correctly
 i.e. Your belief about A given that you know B
15
K-means, C-means, etc Q-learning, etc
Recall, Precision
9
yes)YoungyeseP(Toothach
00003.04.0
yes)ityyes)xP(Cavyyes| CavitYoungyeseP(Toothach
)H|P(a x )P(H argmax H
Evaluation of Learning Model
X2  X2  X2  X2 
 It might rain tonight
1. TRAIN:
A
Slide 71
Loop:
 e.g. Clustering, Anomaly Detection,
60
Metrics
// 1. training
NB
Slide 43
Slide 17
X5  X5  X5  X5 
c1: COOKING c2: SPORTS
Cavity 0.108 0.012 0.072 0.008
25
41
 E1: today, there's a beautiful sunset
Image Classification
X6  X6  X6  X6 
Slide 48
Slide 63
X4  X4  X4  X4 
31
 Probability of an event given that you know that B is
evidence  hypothesis
Today
ether_and_B_togcount_of_A
 ex: out of all people who have the meningitis … how many have red
7
doc4:   "cheap book sale, not meds"
→ Worksheet #3 (“Machine Learning System Evaluation”)
// 2. testing a new document D
X500  X500  X500  X500 
defined class
Getting the Probabilities
email 1 3 2 5 1 0 1 SPAM
Standard Methodology
Slide 12
 P(A|B) = 0.8 P(rain today| cloudy) = 0.8
Slide 32
Reinforcement

1 )c in w of (frequency
…  … …  … 
for all classes ci   // ex. ham or spam
C6 0 0 5 0 10 85  100
for all classes ci
A BA∩B
X6  X6 
 often used as a baseline algorithm before trying other methods
Linear Regression,
iji
data
ii
Slide 10
Slide 64
 if the occurrence of one of them does not influence the
temp
conditional probabilities
 Character recognition: P(character | bitmap)
 so often represented as a feature/attribute vector
Slide 13
Slide 20
 But how do we get the data ?
X7  X7 
offer money viagra laptop exam study category
 Task: classify new digits into one
 if we really do the product of probabilities…
(data analytics)
54
H1 0.7 0.2 0.1
P, improves with experience E.”
 P(the|COOKING) =    P(the|SPORTS) =
experience E with respect to some class of
Slide 34
sunset
Slide 50
Slide 42
Slide 15
1
0.31  .12  .15  .04  .3x.4 .5x.3  .2x.2
Slide 31
 P(A)  = 0.1 P(rain today) = 0.1

actions & discovers errors
 …
Supervised
Slide 1
documents) count(all
in class C Is not in class C
)c in mentscount(docu
 Assume we have 3 hypothesis...
 so instead, we add the log of the probs
Evidence)|isP(Hypothes
 P(A,B) = P(A|B) x P(B) (by chain rule)
a4
Day10 Rain Mild Normal Weak Yes
Types of problems Regression &Classification Association & Clustering Reward based
 DO NOT LOOK AT THE TEST SET  until step 5.
Slide 16
 ex: log(0.01) + log(0.02) + log(0.05) + …
Slide 6
 Prior (or unconditional) probability
 Makes a strong assumption of conditional independence
 Question:
 value = number of times that word appears in the e-mail
 N sets of training texts (1 set for each class)
Training External supervision No supervision No supervision
Slide 29
Reinforcement Learning
 Joint probability

 28 x 28 grayscale images
 P(Montreal, boots) ≠ P(Montreal) * P(boots)
C5 0 0 3 2 92 3  100
 P(meningitis=yes) = 0.00003
• for NB: value of delta for
Slide 69
second for every person on earth.
log(P(hit|SPORTS)) + log(P(the| SPORTS))
c2 : HAM
digits from the American Census
cannot change our belief about A
 i.e. infer new knowledge from observed evidence
non-factual knowledge?
Slide 45
yes)P(spots
Remember this slide…
 P(A) = 1 ⇒ the event A must take place
Day1 Sunny Hot High Weak No
Bayes’ Theorem
26
doc2:  "click here for the best meds"
 P(heat|COOKING) =    P(heat|SPORTS) =
 ex: Text Categorization (spam filtering)
 To make things work in real applications, we often assume
 Text classification: P(sports_news | text)
 E3: today, there's no sunset
X1  X1  X1  X1 
→ If you have spots… you are more likely to have
not hold…

)yesPlayTennis(P )1
= P(A)    x P(B) (by independence)
nb of instances that the
grow from there. By 2020, it is estimated that 1.7MB of data will be created every
Parameters:
~Cavity 0.016 0.064 0.144 0.576
i
n
Slide 57
Ham
nb of instances that
Slide 30
Approach Map labelled input to
e-mail Representation
 Recall: What proportion of the instances in class C are labelled

 Question: how will be the weather tomorrow?
Ex. Application: Spam Filtering
Machine Learning
 what if we have a P(wi|cj) = 0…?
72
But since we only care about ranking the hypothesis…
conditionally independent
...
)H|P(E x )P(H argmaxH ii
69
Hyper-parameters:  parameters
Slide 62
 Given that C is true, then any evidence about B
 eg. when data set is unbalanced  Target system 1
2. Validation set (~20%)
46
Slide 25
 to solve this: we assume that every word always appears at
42
)H|P(E x )P(H
you did not use for training) for which you know the correct
 Score(SPORTS)= log() + log(P(the|SPORTS)) + log(P(referee|SPORTS)) +
 Split data set into 3 sub-sets
Classification Regression
 This e-mail is most likely spam
ML is widely used in Data Mining

 ex: spam / ham
→ Worksheet #3 (“Email Spam Detection”)
 ex: Medical Diagnosis
of the 10 classes
 and:
Supervised Learning
Types of ML Algorithms
Slide 36
 So Bayesian reasoning:
 task: correctly tag a new e-mail
P(B) x B)|P(A  B)P(A,  so
16
 we soon have numerical underflow…
Slide 51
Day5 Rain Cool Normal Weak Yes
2. Divide collection into training, validation and test set
i2i
summer
 P(kitchen|COOKING) =    P(kitchen|SPORTS) =
2. Naïve Bayes Classifier
a2
es
 How do you know if what you learned is correct?
Example 4
8
 Times, the probability that C occurs, assuming that A and B have
Bayes’ Reasoning
“A computer program is said to learn from
 We typically want to know: P(Hypothesis | Evidence)
weather
28
basic values learned by the ML
least once (or a smaller value)
• for DTs: features to split
Error Analysis
ll_Bcount_of_a
B)P(A,  B)|P(A 
Assume we only have 1 hypothesis
90% !
P(E2 |H1)
Follow trail and error
anyways!
is)P(Hypothes)Hypothesis|P(Evidence
Another Application:
toothache young cavity
spots
 Times, the probability that B occurs, assuming that A occurred
The machine is trained
?
 P(head, head) = 1/2 * 1/2 = 1/4
Day8 Sunny Mild High Weak No
j
Association Rule Mining
 each e-mail in the training set is tagged with the correct
 beautiful sunset? clouds? temperature? summer?, …
C1 C2 C3 C4 C5 C6 … Total
Unsupervised
ML outside of AI
COOKiNG SPORTS
Conditional Probability
 Assume now 2 pieces of evidence:
 so we can drop it
class C
 Two events A and B are independent:
for all words wj in the vocabulary
 …
 Use a confusion matrix / contingency table
 each feature = 256 greyscale value
 The evidence is typically represented by many
• for ANNs: weights
are in class C
COMP 6721
doc1:  "cheap meds for sale"
5. Measure performance with the test set
the model identified as
yes)P(Youngyes) eP(Toothach
doc1:   … ball… heat…
yes )Youngyes ) x P(eP(Toothach
 In real life:
Day13 Overcast Hot Normal Weak Yes
P(A1, A2, A3, A4, ..., An)
spots?
B)P(A  B)|P(A 
occurrence of the other
X7  X7  X7  X7 
correctly?
20
)H|a(P estimate ij
 ex: living in Montreal & tossing a coin
 that is often incorrect
Slide 22
 doc6:  “the cheap book”
17
true       (B = some evidence)
 P(Cavity | Toothache) = 0.12
yes)yyes| CavitP(Youngyes)yyes| CaviteP(Toothachyes)P(Cavity
occurred
th
:assumption ceindependen lconditiona with
 when all classes are equally important and represented
)noPlayTennis(P )2
Day2 Sunny Hot High Strong No
34
email 2 1 1 0 5 4 3 HAM
Slide 44
 But… P(E)
2
B)P(A,  A)|P(B 
5
 Features: each pixel is used as a
Over 2.5 quintillion bytes of data are created every single day, and it is only going to
Slide 7
C3 0 1 94 2 1 2  100
P(Toothache ∩ Cavity ∩ Young)
 The assumption of conditional independence, often does
(learns by reacting to
 P(Hi | E2) ?
Chain Rule
So?
→ Worksheet #3 (“Bayes’ Theorem”)
Slide 49
Popular
12
)H|P(E x )P(H  )E|P(H
Association Clustering
 Diagnostic systems: P(Disease | Symptoms)
48
19
 H1: weather will be nice P(H1) = 0.2
P(A) x A)|P(B  B)P(A,  so
person
are in  class C and that
Slide 28
?yes)Youngyescheyes|ToothaP(Cavity 
score(ci) = score(ci) x P(wj | ci)
email 3 0 3 2 1 0 1 SPAM
 is the same for all possible Hi  (and is hard to gather anyways)
evidence

27
All instances that
)yesPlayTennis|strongWind(xP)yesPlayTennis|highHum(xP)yesPlayTennis|coolTemp(xP)yesPlayTennis|sunnyOutlook(P x
 P(ball|COOKING) =       P(ball|SPORTS) =
 500,000 words in Cooking
 P(A) = 0 ⇒ the event A will never take place
49
Cavity 0.04 0.06
Bureau employees and American
 Each set is already tagged by the class name
Slide 4
new email 2 1 0 1 1 2 ?
 Out of n hypothesis…
29
)H(P estimate i
Slide 59

= P (∩Ai)
60.05/3)noPlayTennis|sunnyOut(P
→ Worksheet #3 (“AI Weather Prediction”)
Accuracy  450/500 =
Slide 24
meningitis than if we don’t know about you having
observations
 75,000 docs in Sports
A∩B

 BUT:
3. Evaluation
~80%
 Where did the learner go wrong ?
 MNIST dataset
smoothing,
doc75000:   goal… injury …
Digit Recognition
C2 0 93 3 4 0 0  100
24
Slide 39
prior probabilities P(Hi)
Slide 19
 Rolling two dice (together):
 The probability that A occurs
47
 These 2 pictures are very likely of the same
method
 in most applications, you just count from a set of
environment by producing
Be Careful: Smooth Probabilities
possible combinations of variables
doc1:   … stove… kitchen… the… heat
 basis of many spam filters
A1,...,An take place
 P(Cause | Side Effect)… P(misaligned brakes| squeaky wheels)

33
 problem:
Naive Bayes Classification
 probability of 2 heads in a row:
33.09/3)yesPlayTennis|strongWind(P
 But Naïve Bayes works very well in many applications
1. Introduction to ML
yes) Youngyes hacheyes | TootP(Cavity
 Predict the weather tomorrow based on tonight‘s sunset...
classes assigned by the learner
 e1 = <sunset:beautiful, clouds:no, temp:high, summer:yes>
classifier, because we will count the number of words, as opposed to just
• for ANNs: nb of hidden layers, nb
0206.0
 ex: sports, recreation, politics, war, economy,…
for all classes ci // ex. ham or spam
 With 3 events, the probability that A, B and C occur is:
36.014/5)noPlayTennis(P
 feature = actual words in the e-mail

Assume:
 P(Montreal, head) = P(Montreal) * P(head)
 we want to find the most probable Hi given the evidence E
C4 0 1 3 94 2 0  100
B)P(AB)|P(A 
correct class
 If A and B are independent, then:
 ex: out of all people who have red spots… how many have meningitis?
)H|strongP(Wind x )H|highP(Humidity x
 denoted P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
features / evidence / X f(X)
36
on unlabeled data
63
 we can do probabilistic inference
38
 training set: 60,000 examples
→ Worksheet #3 (“Joint Probabilities”)
 E2: today, there's a average sunset
Slide 8
 H2: weather will be bad P(H2) = 0.5

 surprisingly very effective on real-world tasks
doc5:   "here is the book for you"
compute
X500  X500 
 argmaxcj log(P(cj)) + Σ log(P(wi|c))
P(Toothache ∩ Cavity)
Naive Bayes Classifier
H
c1 : SPAM
Slide 58
32
 ex: 0.01 x 0.02 x 0.05 x …
Slide 66
C are actually correct?
Comments on Naïve Bayes Classification
log(P(hit|COOKING)) + log(P(the|COOKING))
14
Day14 Rain Mild High Strong No
Learning
 Sum Rule
C1 94 3 0 0 3 0  100
Slide 65
Machine Learning Process
attributes/features
44
Slide 27
 300,000 words in Sports
 Two events A and B are conditionally
Representing the Evidence
Artificial Intelligence:
51
word accident given the class SPORTS
obtained
no)X(PlayTennis:answer 
a) For each hypothesis Hi
Slide 18
yes)spots|yesisP(meningit
)c in w of (frequency
)noPlayTennis|strongWind(xP)noPlayTennis|highHum(xP)noPlayTennis|coolTemp(xP)noPlayTennis|sunnyOutlook(P x
 ex. the word "dumbo" never appeared in the class SPAM?
 Accuracy
Day11 Sunny Mild Normal Strong Yes
Now we have decomposed the joint probability distribution into much
tasks T and performance measure P if its
po
independent given C:
A+B
 when one class is more important than the others
Slide 38
etc
Day3 Overcast Hot High Weak Yes
for all words wj in the D
of nodes per layer…
 In reality, we may have dozens, hundreds of variables
 given:
)H|P(a x )P(H argmax  )H|a,...,a,a,aP( x )P(H argmax)H|P(E x )P(H argmax
 Categorization: P(Category | Features of Object)
 with strong (naive) independence assumption
 Rolling two dice one after the other, first dice rolled 1:
X2  X2 
 there are 28x28 = 784 features
Slide 14
doc2:   … kitchen… pasta… stove…
H2 0.3 0.3 0.4
https://en.wikipedia.org/wiki/MNIST_database

66
…
(that should have
45
yes)ityyes)xP(Cavyes|CavityYoungyeseP(Toothach
identified as class C
choose c* = with the greatest score(ci)
doc2:   … the… referee… player…
 when one class C is more important than the others
w1 w2 w3 w4 w5 w6
52
Day Outlook Temperature Humidity Wind Play Tennis
Conditional Probability (con’t)
vocabulary of size c in words of number total
instance is NOT in class C C D
58
 However P(Evidence | Hypothesis) is easier to gather
clouds
)P(E
 Training – Unsmoothed  / Smoothed  probs:
 ex: living in Montreal & wearing boots

 Posterior (or conditional) probability
the concept Machine Learning:
score(ci) = P(ci)
Types of ML Problems
Definition The machine learns by
40
Slide 61
is
 And 1 piece of evidence with 3 possible values
P(Ex|Hi) E1 E2 E3
Slide 60
)P(Evidence
 Ex. with 16 binary variables, we would need 216 entries
• for NB: prior & conditional
H3 0.4 0.4 0.2
 I can’t read this character, but it looks like a “B”
Example 1
nb of instances that are in
 test set: 10,000 examples.
Day6 Rain Cool Normal Strong No
 A patient complains about Toothache and is Young…
Why Machine Learning?
category.
Independent Events
… …
Slide 3
22.09/2)yesPlayTennis|sunnyOut(P
13
65
Slide 11
)H|P(E x )P(H  argmax  E)|P(H argmax  H ii
)H|coolP(Temp x )H|sunnyP(Outlook x )P(H argmax
 P(Disease | Symptoms)… P(meningitis | red spots)
 Joint probability distribution:
Accuracy  450/500 = 90% ! 498/500 = 99.6% 498/500 = 99.6%
no][yes,H
 Goal: Given a new instance X=<a1,…, an>, classify as Yes/No
 Naïve Bayes: Assumes that the attributes/features are
Reward Based

smaller pieces…
 when all classes are equally important and represented
 Task: classify e-mails (documents) into a pre-

 P(A, B |  C) = P(A | C) x P(B | C).
count_of_AP(A) 
X1  X1 
Day9 Sunny Cool Normal Weak Yes
 We cannot have a table with the probability of all
P(A)
environment)
Slide 5
22
05.0
iiii
 If you have red spots on your face, you might have
Spam
Combining Evidence
Slide 46
 Suppose, we know that
argmaxH
X3  X3  X3  X3 
 100,000 docs in Cooking
Precision =
 what is P(Cavity | Toothache ∩ Young) ?
 0 ≤ P(A) ≤ 1
1st estimate the probabilities from the training examples:
Learning
X3  X3 
yes yes ?
performance at tasks in T, as measured by
discover output
events A1,...,An takes place
instance is in class C A B
 should it be classified as HAM or SPAM?
high school students
class C and that the model
Example 4 evidence
classify the new case: X=(Outlook: Sunny, Temp: Cool, Hum: High, Wind: Strong)
Slide 54
 P(referee|COOKING) =    P(referee|SPORTS) =
used to set up the ML model. eg.
Accuracy
e1 beautiful no high yes  Nice

4.05/2)noPlayTennis|rainOut(P
11
55
B)P(A,
 Fast, easy to apply
 H3: weather will be mixed P(H3) = 0.3
P(B)
Be Careful: Use Logs
feature so:
 A simple probabilistic classifier based on Bayes' theorem
= 99%
Slide 56
 denoted P(A∩B) or P(A,B)
33.09/3)yesPlayTennis|rainOut(P

00024.0
60.05/3)noPlayTennis|strongWind(P
…
p(w1|c1) p(w2|c1) p(w3|c1) p(w4|c1) p(w5|c1) p(w6|c1)
 a.k.a. Knowledge Discovery in Databases (KDD)
68
 c2: HAM
X5  X5 
Application of Bayesian Reasoning
 so if a text contains the word "dumbo", the class SPAM is
56
 e1 = <a1, … , an>
 some variables are independent…
Strictly speaking, what we will see is called a Multinomial Naïve Bayes
50
 argmaxcj P(cj) P P(wi|cj)
parameters* to improve performance
and rewards
P(A|B) x P(B) = P(B|A) x P(A)
 ∑i P(Ai) = 1 ⇒ one of the events Ai will take place
probabilities
:assumption ceindependen with
39
1. Actual training set (~80%)
a3.
Day4 Rain Mild High Weak Yes
compute
 then P("dumbo"| SPAM) = 0
 Assume:

known output

30
Slide 47
 With n events, we can generalize to the Chain rule:
Naïve Bayes Algorithm
0053.0
…
Supervised
In reality, the instance is…
(task-driven)
1. Collect a large set of examples (all with correct classifications)
59
offer money viagra laptop exam study category
 But P(Hypothesis| Evidence) is hard to gather
model. eg.
= P(A1) × P(A2|A1) × P(A3|A1,A2) × ... × P(An|A1,A2,A3,…,An-1)
A B
62
 i.e. the features/attributes are conditionally independent
Slide 26
 Image processing: P(face_person | image features)
 Speech recognition: P(words | acoustic signal)
~Cavity 0.01 0.89
 Dataset
Slide 41
4
Example 3
Unsupervised Learning
)H|P(E x )P(H argmax
model labelled as class C
Slide 33
the measles
yes)isP(meningit x yes)meningitis|yesP(spots
)H|P(E x )P(H  )H|P(E x )P(H )H|P(E x )P(H  )P(E 3232221212
classification
Accuracy                        = 495/500
Remember…
 You run your classifier on a data set of unseen examples (that
 then:
 normally:
Slide 9
Slide 21
Slide 68
 P(spots=yes | meningitis=yes) = 0.4
 P(spots=yes) = 0.05

 P is a probability function:
a1
ji
10
3. Apply learning algorithm to training set to learn the parameters
P(E)
Combining Evidence
b) For each attribute value aj  of each instance (evidence)
 fast, simple
without any guidance
So what?
 each e-mail is represented by a vector of feature/value:
Type of data Labelled data Unlabelled data No pre-defined data
4. Measure performance with the validation set, and adjust hyper-
2i 
 compute the probabilities from the training set
 some variables are not independent…
using labelled data
 |V| = 100     vocabulary = {ball, heat, kitchen, referee, stove, the, ... }
E)H P(E)|P(H 
yes)vityyes)x P(Cayes|CavityYoungyes )  xP(yes|CavityeP(Toothach
Algorithms
64.014/9)yesPlayTennis(P
iii
 P(A) + P(~A) = 1
that events are independent
doc3:  "book your trip"
21
 P(Cavity | Young) = 0.18
An agent interacts with its
 Probability of an event before any evidence is
 So we choose the Hi with the largest P(Hi|E)
)c|P(w 
64
3
 P(A,B) = P(A) x P(B)
p(w1|c2) p(w2|c2) p(w3|c2) p(w4|c2) p(w5|c2) p(w6|c2)
43
 Spam filter: P(spam_message | words in e-mail)
 ex: customer email  order, complaint, support request, ...→
67
6
 With 2 events, the probability that A and B occur is:
→ Worksheet #3 (“AI Fraud Detection”)
ji c in words of number total
instance
)H|P(a x )P(H  argmax
Slide 52
 How do we represent and reason about
