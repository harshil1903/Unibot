
1 1 0
Alison
weights: ∆wi = η(T −O)xi
δh ← g′(xh)× Errh = Ok(1−Ok)×
1 + e−(
Step 4. Now we can update our weights using
#2
COMP6721 Worksheet: Neural Networks Winter 2022
∑
starting from the output neuronO5: δ5 = O5(1−O5)×(O5−T5) =
1, if ~x · ~w ≥ threshold
1
Neural Network for XOR. To learn a non-linearly separable function like XOR, we’ll use a neural network
#4
=
• ∆w14 =
Perceptron. Calculate your first neuron activation for the Perceptron (only 100 billion−1 more to go!):
Assume we use the sign function with a threshold
Richard 0.2 0.2 0.2 0.2
are initialized randomly as shown in the table.
δk ← g′(xk)× Errk = Ok(1−Ok)× (Ok − Tk)
• ∆w24 =
one and has its own weight (here w3). Weight changes ∆wi now take the input value xi into account. We
Here, we assume a constant learning rate η = 0.1:
Worksheet #5: Neural Networks
(our learning rate). T is the expected output and O the output produced by the perceptron. Remember to
want the perceptron to learn the two-dimensional data shown on the right:
• w24 = w24 + ∆w24 =
0, otherwise
x1 x2 x1 XOR x2
wjixj
Data w1 w2 w3 f(~x) ok?
0 1 1
Activation function:
with a hidden layer (the weights have been initialized randomly):
O3 = O4 = O5 =
Step 2. The next step is to calculate the error
• w14 = w14 + ∆w14 =
Perceptron Learning. Ok, so for the first training example, the perceptron did not produce the right
Step 1. Compute the output for the three neurons O3, O4 and O5 for the input (x1 = 1, x2 = 1):
wij ← wij + ∆wij , where ∆wij = −ηδjxi
• w45 = w45 + ∆w45 =

∑
• ∆w45 =
Step 3. Now we calculate the error terms for the hidden layer:
k∈outputs
{
only update weights for active connections (i.e., with non-zero input):
1 0 1
Student w1 w2 w3 w4 f(~x) ok?
(use a threshold of 0.55):
Delta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is always
• Θ5 = Θ5 + ∆Θ5 =
COMP 6721 Applied Artificial Intelligence (Winter 2022)
• ∆Θ5 =

#1 0.75 0.5 -0.6
of 0 and a learning rate η = 0.2. The weights
Apply the generalized delta rule for updating the
#3
j wjixj)
0 0 0
• δ3 = O3(1−O3)δ5w35 =
Alan
• δ4 = O4(1−O4)δ5w45 =
For the two neurons (3), (4) in the hidden layer:
j
wkhδk
And finally update the weights (wij ← wij + ∆wij):
output. To learn the correct result, it has to adjust the weights: ∆w = η(T −O), where we set η = 0.05

Oi = sigmoid
f(~x) =
