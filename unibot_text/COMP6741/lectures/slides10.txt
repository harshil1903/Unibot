
Updating the weights
Word Embeddings with
• then, compute error and propagate backwards, adjusting weights until input layer is
• It can not learn data that is not
Continuous Bag Of Words (CBOW)
10.33
Example
Example Neural Network in Keras
• Yes: don’t change any weights
• E.g., for “whisper”, we can generate the following 2-grams and 3-grams
You can download pre-trained word embeddings for many domains:
Perceptron uses supervised learning:
CBOW Word2vec network
(different) input words to similar output vectors
“Man is to Woman what King is to _____?”
Word order is ignored
Now we can do math with word vectors:
10.53
dog banana 0.24327646
wh, whi, hi, his, is, isp, sp, spe, pe, per, er
• Result vector (with 100s of dimensions) is not going to match any other word
Training your own Word2vec model using gensim
• Using the Pima Indians Diabetes dataset: predicting the onset of diabetes
Introduction
• first, apply input and propagate forward until output layer is reached
10.59
# split into input (X) and output (y) variables
Epoch 5/150
Ten CBOW 5-grams from sentence about Monet
10.38
10.47
Y sigmoid =
Subsampling: Sample words according to their frequencies (no stop word removal
Epoch 4/150
Not quite. . . so far, we only used a single neuron.
10.6
Epoch 7/150
yeti − snow + economics = homo economicus
Training multi-layer neural networks
err(x) = |y − f (x)|
the target word
10.35
• look at each training sample
One-Hot Vectors
10.45
• We can use a network of neurons to also learn non-linearly separable data!
Updating weights (I)
10.32
“cat” and “house”
# fit the keras model on the dataset
10.46
Mathematical Perceptron
the big dog
dataset = loadtxt(’pima-indians-diabetes.data.csv’, delimiter=’,’)
Answer analogy questions
# define the keras model
y = dataset[:,8]
• Unsupervised learning (using a so-called autoencoder)
10.4
Today
• Find closest results (e.g., using cosine similarity) for the answer
10.57
The softmax function σ takes as input a vector of K real numbers, and normalizes it
768/768 [==============================] - 0s 87us/step - loss: 0.8344 - accuracy: 0.5964
Required
• intuitively: minimize cost function representing the error of the network
10.56
banana banana 1.0
~w = [w0,w1, ...,wn]
• We’re just using the weights as our word embeddings
Doc2vec Training
10.7
Generative Models
10.15
Idea
0.5586
→ Worksheet #9: Task 1
10.8
10.40
1
• spaCy comes with word vector models (shown later)
• So we will get similar word vectors for words that have a different surface form,
• often used as activation function in the output layer of a neural network
Activation function:
Training input and output example for the CBOW approach
Perceptron Learning
• So the neural network has to learn weights for the hidden layer that map these
0 0 1
Enhancements & Optimizations
Perceptron
Training input and output example for the skip-gram approach
Perceptron Details
from keras.layers import Dense
from keras.models import Sequential
Using Word Vectors with spaCy
Backpropagation
10.58
https://concordiauniversity.on.worldcat.org/oclc/1102387045
X =
Fasttext
Keras & TensorFlow
• Basic idea like before: show input, compute output, determine error, and adjust
Perceptron
In other words...
768/768 [==============================] - 0s 89us/step - loss: 0.6254 - accuracy: 0.6810
• Math with word vectors
Vector dimensionality = Vocabulary size
What can a single Perceptron learn?
king − man + woman = queen
Which one to use?
Linearly Separable Data
Weights vector:
10.49
X = dataset[:,0:8]
0*topic[’cityness’]
CBOW: (continuous-bag-of-words) predicts output word from nearby (input)
...
Pros & Cons
Word Vectors with spaCy
• Introduction to Neural Networks
cat dog 0.80168545
• Two dimensions: line,
Perceptron:
Training: Ten 5-grams from the sentence about Monet
for token1 in tokens:
>>> word_model.distance(’man’, ’nurse’)
COMP 474/6741, Winter 2022
Document vectors with Doc2vec
Softmax
• We can now deal with unseen words, misspelled words, partial words, etc.
# load the dataset
• Going through all training examples once is called an epoch
We could then use these vectors for semantic word math, e.g., to answer analogy
10.37
questions like:
Word Vectors with spaCy
Softmax properties
Epoch 9/150
Bag-of-Words Model
for token2 in tokens:
https://en.wikipedia.org/wiki/Curse_of_dimensionality
Lecture 10
import spacy
So what, it’s useless?
• Slide a rolling window across a sentence to select the surrounding words for
negative samples to decide which weights to update
Neural Network example for the skip-gram training (2/2)
dog dog 1.0
.2*topic[’cityness’]
Diastolic blood pressure (mm Hg)
three dimensions: plane, etc.
Softmax function
→ Worksheet #9: Task 6
10.10
fasttext.cc
• Goal: predict surrounding window of words based on input word
.1*topic[’animalness’] +
nlp = spacy.load("en_core_web_lg") # make sure to use larger model!
So an AI using these word vectors will now have a gender bias!
Backpropagation
• Two different words that have a similar meaning will have similar context words
Your word vectors represent what is in your corpus:
Finding word vectors near the result
Document vectors with Doc2vec
Department of Computer Science
.5*topic[’animalness’] -
→ Worksheet #9: Task 4
Manning Publications Co., 2019.
# compile the keras model
• Synonyms (e.g., “inflammable” and “flammable”) should have nearly identical word
0
10.44
This was pointed out in a famous book by
This does not solve the disambiguation problem: there will be one word vector for
Keras & TensorFlow
• Bias: additional input that is always “1”
Convex Error Curve
model.add(Dense(1, activation=’sigmoid’))
• Released in 2013 (then working at Google) as Word2vec
Computing the answer to the soccer team question
model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[’accuracy’])
Epoch 2/150
Using a pre-trained model
768/768 [==============================] - 0s 86us/step - loss: 0.5269 - accuracy: 0.7096
Notes and Further Reading
Epoch 10/150
10.13
Input vector:
10.28
Epoch 150/150
1 0 0
10.48
→ Worksheet #9: Task 2
Skip-gram: predict the context of words (output words) from an input word
Using TensorFlow backend.
∗[Marvin Minsky and Seymour Papert: Perceptrons: an introduction to computational geometry, MIT Press, 1969]
• Skip-gram is an n-gram with gaps
10.41
dorm − students = bachelor pad
Approaches
• Various models trained on medical documents, Harry Potter, LOTR, ...
languages from Abkhazian to Zulu
• Skip-gram approach works well with small corpora and rare terms (more
{
Say hello to one of your neurons
• (that’s a common trick in using neural networks)
Meaning of the text is lost
10.60
10.42
Neural Networks
cat banana 0.2815437
“bank”, including both “river bank” and “financial bank” contexts.
10.9
Conversion of one-hot vector to word vector
The ’bias’ unit & weight
• Form of supervised learning like Perceptron training
• w ′i = wi + η · (label − predicted) · xi
https://graceavery.com/word2vec-fish-music-bass/
Hidden weights are our word vectors
[LHH19] Hobson Lane, Cole Howard, and Hannes Max Hapke.
Towards better ‘word vectors’
Geometry of Word2vec math
• learning is done in two phases
Word Embeddings with Word2vec
are trained on large corpora (e.g., Google’s 100 billion words news feed)
.1*topic[’cityness’]
Word Embeddings with Word2vec
10.17
Bag-of-Words (BOW) Model
Why does this work?
Epoch 6/150
Perceptron vs. Biological Neuron
Concordia University
(label: training example, predicted: calculated output)
10.16
Natural Language Processing in Action.
• Building word vectors (word embeddings)
• iterating backwards from output layer to input layer, updating weights
Nonconvex Error Curve
for words like “a”, “the”) – similar to idf in tf-idf
768/768 [==============================] - 0s 80us/step - loss: 0.6072 - accuracy: 0.6745
• Capture semantics of words
~x · ~w =
10.36
10.20
barn − cows = garage
10.29
Nonlinearly Separable Data
Multi-layer neural networks with hidden weights
• Example: the XOR function
10.12
Automatic computation of word vectors
10.30
10.55
Word vectors for ten US cities projected onto a 2D map
(e = Euler’s number ≈ 2.71828)
• All words within the sliding window are considered to be the content of the
model.fit(X, y, epochs=150, batch_size=10)
• Dense vectors (smaller dimensions, fewer 0’s)
but similar (or related) semantics
network for a single input–output example
https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
σ(z)j =
• “normalizes” vector to a [0..1] interval, where all values add up to 1
• Google’s Word2vec model trained on Google News articles
n∑
0 1 0
2 Neural Networks 101
Turn words into numbers, so we can feed them into a neural network.
dog cat 0.80168545
based on diagnostic measures, like 2-Hour serum insulin (mu U/ml) and
road − ocean + car = sailboat
cat cat 1.0
word_vector[’NYC’] = -.2*topic[’petness’] +
• A single Perceptron can learn
References
Other cost functions: mean squared error, cross-entropy, ...
Word2vec
Forward step
Output
word_vector[’apple’] = 0*topic[’petness’] -
Neural Networks 101
linearly separable
Idea: train on character n-grams, not on word n-grams:
• Facebook’s fastText model (for 294 languages)
banana dog 0.24327646
10.3
We can encode the sentence The big dog as a series of three-dimensional vectors:
word_vector[’love’] = .2*topic[’petness’] -
Training a Word2vec model
into a probability distribution consisting of K probabilities proportional to the
.5*topic[’cityness’]
• [LHH19, Chapters 5, 6] (Neural Networks, Word Vectors)
Now what?
“Who is to physics what Louis Pasteur is to germs?”
768/768 [==============================] - 0s 80us/step - loss: 0.6121 - accuracy: 0.6745
word_vector[’dog’] = .3*topic[’petness’] +
0.7453
• So the output vector for these different words have to be similar
• CBOW shows higher accuracies for frequent words and is faster to train
f (~x) =
3D vectors for six words about pets and NYC
word_vector[’lion’] = 0*topic[’petness’] +
linearly separable data
exponentials of the input numbers:
wi · xi
• algorithm performs gradient descent to try minimizing the error
10.5
• In 2012, Thomas Mikolov (intern at Microsoft) trained a neural network to
Outline
Perceptron Learning Rule
Problems with the Bag-of-Words Model
4 Notes and Further Reading
• Why? Consider the case that all xi = 0, but we need to output 1
Based on how much they contributed to the error:
10.11
• η is called the learning rate (e.g., η = 0.2)
xiwi − θ
0, otherwise
See https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
Notes and Further
~x = [x0, x1, ..., xn]
Word Vector Requirements
fish + music = bass
Generate answers to analogy questions like:
• Open source project by Facebook research; pre-trained models for 294
10.61
10.14
3 Word Embeddings
1 Introduction
10.51
Note
https://concordiauniversity.on.worldcat.org/oclc/1102387045.
• Unlike in the BOW model, we do not lose information
(a “1” means on, or hot; a “0” means off, or absent.)
10.1
René Witte
10.43
model = Sequential()
reached
• The (cosine) distance between “cat” and “dog” should be smaller than between
vectors
Dot product:
weights to reduce error
→ Worksheet #9: Task 3
• First proposed in 1969, but not used until 1980s because of high computational
• Notation differs in the literature, but idea is always the same
Cost function
Error between truth and prediction:
from numpy import loadtxt
• Word vectors (a.k.a. word embeddings) typically have 100-500 dimensions and
Reading Material
768/768 [==============================] - 0s 87us/step - loss: 0.6315 - accuracy: 0.6888
k=1 e
Introduction
Summary of Chatbot Approaches
768/768 [==============================] - 0s 93us/step - loss: 0.7119 - accuracy: 0.6510
vector exactly
768/768 [==============================] - 0s 84us/step - loss: 0.6358 - accuracy: 0.6602
Neural Networks 101
10.26
One-Hot Vectors
demands
word_vector[’cat’] = .3*topic[’petness’] +
Basic Perceptron (Franz Rosenblatt, 1957)
10.50
• output correct?
Fasttext
768/768 [==============================] - 0s 85us/step - loss: 0.6086 - accuracy: 0.6615
• We’re not actually using the neural network we trained
Google News Word2vec 300-D vectors projected onto a 2D map using PCA
Task
• Not practical for long documents
10.34
that has 1 (one) for the word, else 0 (zero).
ezj∑K
Skip-gram
tokens = nlp("dog cat banana")
appearing around them
“Japan is to Sushi what Germany is to _____?”
• No: update the weights that were activated
→ Worksheet #9: Task 5
Minksy & Papert in 1969∗
Neurons in backpropagation networks compute the net weighted input like the
words
10.25
print(token1.text, token2.text, token1.similarity(token2))
banana cat 0.2815437
10.24
model.add(Dense(12, input_dim=8, activation=’relu’))
768/768 [==============================] - 0s 87us/step - loss: 0.6776 - accuracy: 0.6484
Epoch 3/150
and Software Engineering
By calculating ~w(’Louis Pasteur’) −~w(’germs’) +~w(’physics’)
10.19
Learning the weights
See https://graceavery.com/word2vec-fish-music-bass/ for more fun examples
• compute the gradient of the loss function with respect to the weights of the
Various Improvements
i=1
The ‘Curse of Dimensionality’
Epoch 1/150
10.31
Copyright 2019 by Manning Publications Co., [LHH19]
Hand-crafting Word Vectors (6 words, 3 dimensions)
Weighted input
Activation function
• E.g., Animal-ness, Place-ness, Action-ness. . .
CBOW
But here we use a sigmoid activation function
Updating weights (II)
Paris − France + Germany = Berlin
1 + e−X
Doc2vec
f (x) = min
With n-dimensional vectors of {0,1}, we can represent each word in our vocabulary
Frequent Bigrams: Pre-process the corpus and add frequent bigrams as terms
Word Embeddings
Word vectors can be biased
Document vectors with
Backpropagation rule
1, if ~x · ~w ≥ threshold
10.27
err(xi)
Reading
.1*topic[’animalness’] -
10.39
Neural Networks & Word Embeddings
Neural Network example for the skip-gram training (1/2)
training data due to the network structure)
>>> word_model.distance(’woman’, ’nurse’)
10.18
zk
Epoch 8/150
Bag-of-Words Model
10.2
predict word occurrences near each target word
(e.g., “New York”, “Elvis Presley”)
desert − sand + suburbia = driveways
model.add(Dense(8, activation=’relu’))
Cost function you want to minimize:
768/768 [==============================] - 0s 257us/step - loss: 4.7881 - accuracy: 0.6107
10.52
Negative sampling: To speed up training, don’t update all weights, but pick some
Word Embeddings
